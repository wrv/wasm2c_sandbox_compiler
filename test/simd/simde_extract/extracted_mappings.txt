[{
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_load(mem);\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load',
    'x86_sse2_mapping': ['  #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '    return _mm_loadu_si128(HEDLEY_REINTERPRET_CAST(const __m128i*, mem));\n', '  #else\n', '    simde_v128_t r;\n', '    simde_memcpy(&r, mem, sizeof(r));\n', '    return r;\n', '  #endif\n', '}\n']
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    wasm_v128_store(mem, a);\n'],
    'type': 'void',
    'name': 'simde_wasm_v128_store',
    'x86_sse2_mapping': ['  #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '    _mm_storeu_si128(HEDLEY_REINTERPRET_CAST(__m128i*, mem), a);\n', '  #else\n', '    simde_memcpy(mem, &a, sizeof(a));\n', '  #endif\n', '}\n']
}, {
    'x86_sse2_mapping': ['  #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '    return\n', '      _mm_setr_epi8(\n', '        c0, c1,  c2,  c3,  c4,  c5,  c6,  c7,\n', '        c8, c9, c10, c11, c12, c13, c14, c15);\n', '  #else\n', '    simde_v128_private r_;\n', '\n', '    r_.i8[ 0] =  c0;\n', '    r_.i8[ 1] =  c1;\n', '    r_.i8[ 2] =  c2;\n', '    r_.i8[ 3] =  c3;\n', '    r_.i8[ 4] =  c4;\n', '    r_.i8[ 5] =  c5;\n', '    r_.i8[ 6] =  c6;\n', '    r_.i8[ 7] =  c7;\n', '    r_.i8[ 8] =  c8;\n', '    r_.i8[ 9] =  c9;\n', '    r_.i8[10] = c10;\n', '    r_.i8[11] = c11;\n', '    r_.i8[12] = c12;\n', '    r_.i8[13] = c13;\n', '    r_.i8[14] = c14;\n', '    r_.i8[15] = c15;\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return\n', '      wasm_i8x16_make(\n', '        c0, c1,  c2,  c3,  c4,  c5,  c6,  c7,\n', '        c8, c9, c10, c11, c12, c13, c14, c15);\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i8x16_make',
    'tbd': ['    int8_t c0, int8_t c1, int8_t  c2, int8_t  c3, int8_t  c4, int8_t  c5, int8_t  c6, int8_t  c7,\n', '    int8_t c8, int8_t c9, int8_t c10, int8_t c11, int8_t c12, int8_t c13, int8_t c14, int8_t c15) {\n']
}, {
    'x86_sse2_mapping': ['  #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '    return _mm_setr_epi16(c0, c1, c2, c3, c4, c5, c6, c7);\n', '  #else\n', '    simde_v128_private r_;\n', '\n', '    r_.i16[0] = c0;\n', '    r_.i16[1] = c1;\n', '    r_.i16[2] = c2;\n', '    r_.i16[3] = c3;\n', '    r_.i16[4] = c4;\n', '    r_.i16[5] = c5;\n', '    r_.i16[6] = c6;\n', '    r_.i16[7] = c7;\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_make(c0, c1, c2, c3, c4, c5, c6, c7);\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i16x8_make',
    'tbd': ['    int16_t c0, int16_t c1, int16_t  c2, int16_t  c3, int16_t  c4, int16_t  c5, int16_t  c6, int16_t  c7) {\n']
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_make(c0, c1, c2, c3);\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i32x4_make',
    'x86_sse2_mapping': ['  #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '    return _mm_setr_epi32(c0, c1, c2, c3);\n', '  #else\n', '    simde_v128_private r_;\n', '\n', '    r_.i32[0] = c0;\n', '    r_.i32[1] = c1;\n', '    r_.i32[2] = c2;\n', '    r_.i32[3] = c3;\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n']
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_make(c0, c1);\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i64x2_make',
    'x86_sse2_mapping': ['  #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '    return _mm_set_epi64x(c1, c0);\n', '  #else\n', '    simde_v128_private r_;\n', '\n', '    r_.i64[ 0] = c0;\n', '    r_.i64[ 1] = c1;\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n']
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_make(c0, c1, c2, c3);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f32x4_make',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_setr_ps(c0, c1, c2, c3);\n', '    #else\n', '      r_.f32[0] = c0;\n', '      r_.f32[1] = c1;\n', '      r_.f32[2] = c2;\n', '      r_.f32[3] = c3;\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n']
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_make(c0, c1);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f64x2_make',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_set_pd(c1, c0);\n', '    #else\n', '      r_.f64[ 0] = c0;\n', '      r_.f64[ 1] = c1;\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n']
}, {
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i8x16_const',
    'tbd': ['      int8_t c0, int8_t c1, int8_t  c2, int8_t  c3, int8_t  c4, int8_t  c5, int8_t  c6, int8_t  c7,\n', '      int8_t c8, int8_t c9, int8_t c10, int8_t c11, int8_t c12, int8_t c13, int8_t c14, int8_t c15) {\n', '    return simde_wasm_i8x16_make(\n', '        c0, c1,  c2,  c3,  c4,  c5,  c6,  c7,\n', '        c8, c9, c10, c11, c12, c13, c14, c15);\n', '  }\n', '#endif\n']
}, {
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i16x8_const',
    'tbd': ['      int16_t c0, int16_t c1, int16_t  c2, int16_t  c3, int16_t  c4, int16_t  c5, int16_t  c6, int16_t  c7) {\n', '    return simde_wasm_i16x8_make(\n', '        c0, c1,  c2,  c3,  c4,  c5,  c6,  c7);\n', '  }\n', '#endif\n']
}, {
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i32x4_const',
    'tbd': ['      int32_t c0, int32_t c1, int32_t  c2, int32_t  c3) {\n', '    return simde_wasm_i32x4_make(\n', '        c0, c1,  c2,  c3);\n', '  }\n', '#endif\n']
}, {
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i64x2_const',
    'tbd': ['      int64_t c0, int64_t c1) {\n', '    return simde_wasm_i64x2_make(\n', '        c0, c1);\n', '  }\n', '#endif\n']
}, {
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f32x4_const',
    'tbd': ['      simde_float32 c0, simde_float32 c1, simde_float32  c2, simde_float32  c3) {\n', '    return simde_wasm_f32x4_make(\n', '        c0, c1,  c2,  c3);\n', '  }\n', '#endif\n']
}, {
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f64x2_const',
    'tbd': ['      simde_float64 c0, simde_float64 c1) {\n', '    return simde_wasm_f64x2_make(\n', '        c0, c1);\n', '  }\n', '#endif\n']
}, {
    'name': 'simde_wasm_i8x16_splat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = a;\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_splat(a);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vdupq_n_s8(a);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_set1_epi8(a);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_i8 = vec_splats(a);\n', '    #else\n']
}, {
    'name': 'simde_wasm_i16x8_splat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = a;\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_splat(a);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vdupq_n_s16(a);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_set1_epi16(a);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_i16 = vec_splats(a);\n', '    #else\n']
}, {
    'name': 'simde_wasm_i32x4_splat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = a;\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_splat(a);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vdupq_n_s32(a);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_set1_epi32(a);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_i32 = vec_splats(a);\n', '    #else\n']
}, {
    'name': 'simde_wasm_i64x2_splat',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_i64 = vec_splats(HEDLEY_STATIC_CAST(signed long long, a));\n', '    #else\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = a;\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_splat(a);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i64 = vdupq_n_s64(a);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE) && (!defined(HEDLEY_MSVC_VERSION) || HEDLEY_MSVC_VERSION_CHECK(19,0,0))\n', '      r_.sse_m128i = _mm_set1_epi64x(a);\n']
}, {
    'name': 'simde_wasm_f32x4_splat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = a;\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_splat(a);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_f32 = vdupq_n_f32(a);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_set1_ps(a);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_14_NATIVE)\n', '      r_.altivec_f32 = vec_splats(a);\n', '    #else\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f64 = vdupq_n_f64(a);\n'],
    'name': 'simde_wasm_f64x2_splat',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_f64 = vec_splats(a);\n', '    #else\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = a;\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_splat(a);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_set1_pd(a);\n']
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_load8_splat(mem);\n', '  #else\n', '    int8_t v;\n', '    simde_memcpy(&v, mem, sizeof(v));\n', '    return simde_wasm_i8x16_splat(v);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load8_splat'
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_load16_splat(mem);\n', '  #else\n', '    int16_t v;\n', '    simde_memcpy(&v, mem, sizeof(v));\n', '    return simde_wasm_i16x8_splat(v);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load16_splat'
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_load32_splat(mem);\n', '  #else\n', '    int32_t v;\n', '    simde_memcpy(&v, mem, sizeof(v));\n', '    return simde_wasm_i32x4_splat(v);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load32_splat'
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_load64_splat(mem);\n', '  #else\n', '    int64_t v;\n', '    simde_memcpy(&v, mem, sizeof(v));\n', '    return simde_wasm_i64x2_splat(v);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load64_splat'
}, {
    'name': 'simde_wasm_i8x16_extract_lane',
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_i8x16_extract_lane(a, lane) HEDLEY_STATIC_CAST(int8_t, wasm_i8x16_extract_lane((a), (lane)))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '  #define simde_wasm_i8x16_extract_lane(a, lane) vgetq_lane_s8(simde_v128_to_neon_i8(a), (lane) & 15)\n', '#endif\n'],
    'x86_sse41_mapping': ['#elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '  #define simde_wasm_i8x16_extract_lane(a, lane) HEDLEY_STATIC_CAST(int8_t, _mm_extract_epi8(simde_v128_to_m128i(a), (lane) & 15))\n'],
    'type': 'int8_t',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  return a_.i8[lane & 15];\n', '}\n']
}, {
    'name': 'simde_wasm_i16x8_extract_lane',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  return a_.i16[lane & 7];\n', '}\n'],
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_i16x8_extract_lane(a, lane) HEDLEY_STATIC_CAST(int16_t, wasm_i16x8_extract_lane((a), (lane)))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_i16x8_extract_lane(a, lane) vgetq_lane_s16(simde_v128_to_neon_i16(a), (lane) & 7)\n', '#endif\n'],
    'type': 'int16_t',
    'x86_sse2_mapping': ['#elif defined(SIMDE_X86_SSE2_NATIVE)\n', '  #define simde_wasm_i16x8_extract_lane(a, lane) HEDLEY_STATIC_CAST(int16_t, _mm_extract_epi16((a), (lane) & 7))\n']
}, {
    'name': 'simde_wasm_i32x4_extract_lane',
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_i32x4_extract_lane(a, lane) HEDLEY_STATIC_CAST(int32_t, wasm_i32x4_extract_lane((a), (lane)))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_i32x4_extract_lane(a, lane) vgetq_lane_s32(simde_v128_to_neon_i32(a), (lane) & 3)\n', '#endif\n'],
    'x86_sse41_mapping': ['#elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '  #define simde_wasm_i32x4_extract_lane(a, lane) HEDLEY_STATIC_CAST(int32_t, _mm_extract_epi32((a), (lane) & 3))\n'],
    'type': 'int32_t',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  return a_.i32[lane & 3];\n', '}\n']
}, {
    'name': 'simde_wasm_i64x2_extract_lane',
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_i64x2_extract_lane(a, lane) HEDLEY_STATIC_CAST(int64_t, wasm_i64x2_extract_lane((a), (lane)))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_i64x2_extract_lane(a, lane) vgetq_lane_s64(simde_v128_to_neon_i64(a), (lane) & 1)\n', '#endif\n'],
    'x86_sse41_mapping': ['#elif defined(SIMDE_X86_SSE4_1_NATIVE) && defined(SIMDE_ARCH_AMD64)\n', '  #define simde_wasm_i64x2_extract_lane(a, lane) HEDLEY_STATIC_CAST(int64_t, _mm_extract_epi64((a), (lane) & 1))\n'],
    'type': 'int64_t',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  return a_.i64[lane & 1];\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_u8x16_extract_lane(a, lane) HEDLEY_STATIC_CAST(uint8_t, wasm_u8x16_extract_lane((a), (lane)))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '  #define simde_wasm_u8x16_extract_lane(a, lane) vgetq_lane_u8(simde_v128_to_neon_u8(a), (lane) & 15)\n', '#endif\n'],
    'type': 'uint8_t',
    'name': 'simde_wasm_u8x16_extract_lane',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  return a_.u8[lane & 15];\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_u16x8_extract_lane(a, lane) HEDLEY_STATIC_CAST(uint16_t, wasm_u16x8_extract_lane((a), (lane)))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_u16x8_extract_lane(a, lane) vgetq_lane_u16(simde_v128_to_neon_u16(a), (lane) & 7)\n', '#endif\n'],
    'type': 'uint16_t',
    'name': 'simde_wasm_u16x8_extract_lane',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  return a_.u16[lane & 7];\n', '}\n']
}, {
    'name': 'simde_wasm_f32x4_extract_lane',
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_f32x4_extract_lane(a, lane) wasm_f32x4_extract_lane((a), (lane))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_f32x4_extract_lane(a, lane) vgetq_lane_f32(simde_v128_to_neon_f32(a), (lane) & 3)\n', '#endif\n'],
    'x86_sse41_mapping': ['#elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '  #define simde_wasm_f32x4(a, lane) _mm_extract_ps(simde_v128_to_m128(a), (lane) & 3)\n'],
    'type': 'simde_float32',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  return a_.f32[lane & 3];\n', '}\n']
}, {
    'arm_neon_a64v8_mapping': ['#elif defined(SIMDE_ARM_NEON_A64V8_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_f64x2_extract_lane(a, lane) vgetq_lane_f64(simde_v128_to_neon_f64(a), (lane) & 1)\n', '#endif\n'],
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_f64x2_extract_lane(a, lane) wasm_f64x2_extract_lane((a), (lane))\n'],
    'type': 'simde_float64',
    'name': 'simde_wasm_f64x2_extract_lane',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  return a_.f64[lane & 1];\n', '}\n']
}, {
    'name': 'simde_wasm_i8x16_replace_lane',
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_i8x16_replace_lane(a, lane, value) wasm_i8x16_replace_lane((a), (lane), (value))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '  #define simde_wasm_i8x16_replace_lane(a, lane, value) simde_v128_from_neon_i8(vsetq_lane_s8((value), simde_v128_to_neon_i8(a), (lane) & 15))\n', '#endif\n'],
    'x86_sse41_mapping': ['#elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '  #if defined(__clang__) && !SIMDE_DETECT_CLANG_VERSION_CHECK(7,0,0)\n', '    #define simde_wasm_i8x16_replace_lane(a, lane, value) HEDLEY_REINTERPRET_CAST(simde_v128_t, _mm_insert_epi8((a), (value), (lane) & 15))\n', '  #else\n', '    #define simde_wasm_i8x16_replace_lane(a, lane, value) _mm_insert_epi8((a), (value), (lane) & 15)\n', '  #endif\n'],
    'type': 'simde_v128_t',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  a_.i8[lane & 15] = value;\n', '  return simde_v128_from_private(a_);\n', '}\n']
}, {
    'name': 'simde_wasm_i16x8_replace_lane',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  a_.i16[lane & 7] = value;\n', '  return simde_v128_from_private(a_);\n', '}\n'],
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_i16x8_replace_lane(a, lane, value) wasm_i16x8_replace_lane((a), (lane), (value))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_i16x8_replace_lane(a, lane, value) simde_v128_from_neon_i16(vsetq_lane_s16((value), simde_v128_to_neon_i16(a), (lane) & 7))\n', '#endif\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['#elif defined(SIMDE_X86_SSE2_NATIVE)\n', '  #define simde_wasm_i16x8_replace_lane(a, lane, value) _mm_insert_epi16((a), (value), (lane) & 7)\n']
}, {
    'name': 'simde_wasm_i32x4_replace_lane',
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_i32x4_replace_lane(a, lane, value) wasm_i32x4_replace_lane((a), (lane), (value))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_i32x4_replace_lane(a, lane, value) simde_v128_from_neon_i32(vsetq_lane_s32((value), simde_v128_to_neon_i32(a), (lane) & 3))\n', '#endif\n'],
    'x86_sse41_mapping': ['#elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '  #if defined(__clang__) && !SIMDE_DETECT_CLANG_VERSION_CHECK(7,0,0)\n', '    #define simde_wasm_i32x4_replace_lane(a, lane, value) HEDLEY_REINTERPRET_CAST(simde_v128_t, _mm_insert_epi32((a), (value), (lane) & 3))\n', '  #else\n', '    #define simde_wasm_i32x4_replace_lane(a, lane, value) _mm_insert_epi32((a), (value), (lane) & 3)\n', '  #endif\n'],
    'type': 'simde_v128_t',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  a_.i32[lane & 3] = value;\n', '  return simde_v128_from_private(a_);\n', '}\n']
}, {
    'name': 'simde_wasm_i64x2_replace_lane',
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_i64x2_replace_lane(a, lane, value) wasm_i64x2_replace_lane((a), (lane), (value))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_i64x2_replace_lane(a, lane, value) simde_v128_from_neon_i64(vsetq_lane_s64((value), simde_v128_to_neon_i64(a), (lane) & 1))\n', '#endif\n'],
    'x86_sse41_mapping': ['#elif defined(SIMDE_X86_SSE4_1_NATIVE) && defined(SIMDE_ARCH_AMD64)\n', '  #define simde_wasm_i64x2_replace_lane(a, lane, value) _mm_insert_epi64((a), (value), (lane) & 1)\n'],
    'type': 'simde_v128_t',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  a_.i64[lane & 1] = value;\n', '  return simde_v128_from_private(a_);\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_f32x4_replace_lane(a, lane, value) wasm_f32x4_replace_lane((a), (lane), (value))\n'],
    'arm_neon_a32v7_mapping': ['#elif defined(SIMDE_ARM_NEON_A32V7_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_f32x4_replace_lane(a, lane, value) simde_v128_from_neon_f32(vsetq_lane_f32((value), simde_v128_to_neon_f32(a), (lane) & 3))\n', '#endif\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f32x4_replace_lane',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  a_.f32[lane & 3] = value;\n', '  return simde_v128_from_private(a_);\n', '}\n']
}, {
    'arm_neon_a64v8_mapping': ['#elif defined(SIMDE_ARM_NEON_A64V8_NATIVE) && !defined(SIMDE_BUG_CLANG_BAD_VGET_SET_LANE_TYPES)\n', '  #define simde_wasm_f64x2_replace_lane(a, lane, value) simde_v128_from_neon_f64(vsetq_lane_f64((value), simde_v128_to_neon_f64(a), (lane) & 1))\n', '#endif\n'],
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_f64x2_replace_lane(a, lane, value) wasm_f64x2_replace_lane((a), (lane), (value))\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f64x2_replace_lane',
    'tbd': ['  simde_v128_private a_ = simde_v128_to_private(a);\n', '  a_.f64[lane & 1] = value;\n', '  return simde_v128_from_private(a_);\n', '}\n']
}, {
    'name': 'simde_wasm_i8x16_eq',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = (a_.i8[i] == b_.i8[i]) ? ~INT8_C(0) : INT8_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i8 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i8), a_.i8 == b_.i8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_eq(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vceqq_s8(a_.neon_i8, b_.neon_i8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi8(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_i16x8_eq',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = (a_.i16[i] == b_.i16[i]) ? ~INT16_C(0) : INT16_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i16 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i16), a_.i16 == b_.i16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_eq(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vceqq_s16(a_.neon_i16, b_.neon_i16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi16(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_i32x4_eq',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = (a_.i32[i] == b_.i32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.i32 == b_.i32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_eq(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vceqq_s32(a_.neon_i32, b_.neon_i32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi32(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u64 = vceqq_s64(a_.neon_i64, b_.neon_i64);\n'],
    'name': 'simde_wasm_i64x2_eq',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = (a_.i64[i] == b_.i64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i64 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i64), a_.i64 == b_.i64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_eq(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi64(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_f32x4_eq',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.i32[i] = (a_.f32[i] == b_.f32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f32 == b_.f32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_eq(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vceqq_f32(a_.neon_f32, b_.neon_f32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_cmpeq_ps(a_.sse_m128, b_.sse_m128);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u64 = vceqq_f64(a_.neon_f64, b_.neon_f64);\n'],
    'name': 'simde_wasm_f64x2_eq',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.i64[i] = (a_.f64[i] == b_.f64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f64 == b_.f64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_eq(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_cmpeq_pd(a_.sse_m128d, b_.sse_m128d);\n']
}, {
    'name': 'simde_wasm_i8x16_ne',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = (a_.i8[i] != b_.i8[i]) ? ~INT8_C(0) : INT8_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i8 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i8), a_.i8 != b_.i8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_ne(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vmvnq_u8(vceqq_s8(a_.neon_i8, b_.neon_i8));\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_i16x8_ne',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = (a_.i16[i] != b_.i16[i]) ? ~INT16_C(0) : INT16_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i16 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i16), a_.i16 != b_.i16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_ne(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vmvnq_u16(vceqq_s16(a_.neon_i16, b_.neon_i16));\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_i32x4_ne',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = (a_.i32[i] != b_.i32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.i32 != b_.i32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_ne(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vmvnq_u32(vceqq_s32(a_.neon_i32, b_.neon_i32));\n'],
    'type': 'simde_v128_t'
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u32 = vmvnq_u32(vreinterpretq_u32_u64(vceqq_s64(a_.neon_i64, b_.neon_i64)));\n'],
    'name': 'simde_wasm_i64x2_ne',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = (a_.i64[i] != b_.i64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i64 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i64), a_.i64 != b_.i64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_ne(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_f32x4_ne',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.i32[i] = (a_.f32[i] != b_.f32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f32 != b_.f32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_ne(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vmvnq_u32(vceqq_f32(a_.neon_f32, b_.neon_f32));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_cmpneq_ps(a_.sse_m128, b_.sse_m128);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u32 = vmvnq_u32(vreinterpretq_u32_u64(vceqq_f64(a_.neon_f64, b_.neon_f64)));\n'],
    'name': 'simde_wasm_f64x2_ne',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.i64[i] = (a_.f64[i] != b_.f64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f64 != b_.f64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_ne(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_cmpneq_pd(a_.sse_m128d, b_.sse_m128d);\n']
}, {
    'name': 'simde_wasm_i8x16_lt',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = (a_.i8[i] < b_.i8[i]) ? ~INT8_C(0) : INT8_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i8 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i8), a_.i8 < b_.i8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_lt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vcltq_s8(a_.neon_i8, b_.neon_i8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_cmplt_epi8(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed char), vec_cmplt(a_.altivec_i8, b_.altivec_i8));\n']
}, {
    'name': 'simde_wasm_i16x8_lt',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = (a_.i16[i] < b_.i16[i]) ? ~INT16_C(0) : INT16_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i16 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i16), a_.i16 < b_.i16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_lt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vcltq_s16(a_.neon_i16, b_.neon_i16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_cmplt_epi16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed short), vec_cmplt(a_.altivec_i16, b_.altivec_i16));\n']
}, {
    'name': 'simde_wasm_i32x4_lt',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = (a_.i32[i] < b_.i32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.i32 < b_.i32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_lt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcltq_s32(a_.neon_i32, b_.neon_i32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_cmplt_epi32(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), vec_cmplt(a_.altivec_i32, b_.altivec_i32));\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u64 = vcltq_s64(a_.neon_i64, b_.neon_i64);\n'],
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed int) tmp =\n', '        vec_or(\n', '          vec_and(\n', '            HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), vec_cmpeq(b_.altivec_i32, a_.altivec_i32)),\n', '            HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), vec_sub(\n', '              HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed long long), a_.altivec_i32),\n', '              HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed long long), b_.altivec_i32)\n', '            ))\n', '          ),\n', '          vec_cmpgt(b_.altivec_i32, a_.altivec_i32)\n', '        );\n', '        r_.altivec_i32 = vec_mergeo(tmp, tmp);\n'],
    'name': 'simde_wasm_i64x2_lt',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = (a_.i64[i] < b_.i64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i64 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i64), a_.i64 < b_.i64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_lt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'x86_sse42_mapping': ['    #elif defined(SIMDE_X86_SSE4_2_NATIVE)\n', '      r_.sse_m128i = _mm_cmpgt_epi64(b_.sse_m128i, a_.sse_m128i);\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      int32x4_t tmp = vorrq_s32(\n', '        vandq_s32(\n', '          vreinterpretq_s32_u32(vceqq_s32(b_.neon_i32, a_.neon_i32)),\n', '          vsubq_s32(a_.neon_i32, b_.neon_i32)\n', '        ),\n', '        vreinterpretq_s32_u32(vcgtq_s32(b_.neon_i32, a_.neon_i32))\n', '      );\n', '      int32x4x2_t trn = vtrnq_s32(tmp, tmp);\n', '      r_.neon_i32 = trn.val[1];\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      /* https://stackoverflow.com/a/65175746 */\n', '      r_.sse_m128i =\n', '        _mm_shuffle_epi32(\n', '          _mm_or_si128(\n', '            _mm_and_si128(\n', '              _mm_cmpeq_epi32(b_.sse_m128i, a_.sse_m128i),\n', '              _mm_sub_epi64(a_.sse_m128i, b_.sse_m128i)\n', '            ),\n', '            _mm_cmpgt_epi32(\n', '              b_.sse_m128i,\n', '              a_.sse_m128i\n', '            )\n', '          ),\n', '          _MM_SHUFFLE(3, 3, 1, 1)\n', '        );\n']
}, {
    'name': 'simde_wasm_u8x16_lt',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u8) / sizeof(r_.u8[0])) ; i++) {\n', '        r_.u8[i] = (a_.u8[i] < b_.u8[i]) ? ~UINT8_C(0) : UINT8_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.u8 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u8), a_.u8 < b_.u8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_lt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vcltq_u8(a_.neon_u8, b_.neon_u8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      __m128i tmp = _mm_subs_epu8(b_.sse_m128i, a_.sse_m128i);\n', '      r_.sse_m128i = _mm_adds_epu8(tmp, _mm_sub_epi8(_mm_setzero_si128(), tmp));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u8 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned char), vec_cmplt(a_.altivec_u8, b_.altivec_u8));\n']
}, {
    'name': 'simde_wasm_u16x8_lt',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = (a_.u16[i] < b_.u16[i]) ? ~UINT16_C(0) : UINT16_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.u16 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u16), a_.u16 < b_.u16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_lt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vcltq_u16(a_.neon_u16, b_.neon_u16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      __m128i tmp = _mm_subs_epu16(b_.sse_m128i, a_.sse_m128i);\n', '      r_.sse_m128i = _mm_adds_epu16(tmp, _mm_sub_epi16(_mm_setzero_si128(), tmp));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u16 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned short), vec_cmplt(a_.altivec_u16, b_.altivec_u16));\n']
}, {
    'name': 'simde_wasm_u32x4_lt',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u32) / sizeof(r_.u32[0])) ; i++) {\n', '        r_.u32[i] = (a_.u32[i] < b_.u32[i]) ? ~UINT32_C(0) : UINT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.u32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u32), a_.u32 < b_.u32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_lt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcltq_u32(a_.neon_u32, b_.neon_u32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_xor_si128(\n', '          _mm_cmpgt_epi32(b_.sse_m128i, a_.sse_m128i),\n', '          _mm_srai_epi32(_mm_xor_si128(b_.sse_m128i, a_.sse_m128i), 31)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u32 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned int), vec_cmplt(a_.altivec_u32, b_.altivec_u32));\n']
}, {
    'name': 'simde_wasm_f32x4_lt',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.i32[i] = (a_.f32[i] < b_.f32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f32 < b_.f32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_lt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcltq_f32(a_.neon_f32, b_.neon_f32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_cmplt_ps(a_.sse_m128, b_.sse_m128);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_f32 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(float), vec_cmplt(a_.altivec_f32, b_.altivec_f32));\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u64 = vcltq_f64(a_.neon_f64, b_.neon_f64);\n'],
    'name': 'simde_wasm_f64x2_lt',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_f64 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(double), vec_cmplt(a_.altivec_f64, b_.altivec_f64));\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.i64[i] = (a_.f64[i] < b_.f64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f64 < b_.f64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_lt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_cmplt_pd(a_.sse_m128d, b_.sse_m128d);\n']
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_gt(a, b);\n', '  #else\n', '    return simde_wasm_i8x16_lt(b, a);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i8x16_gt'
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_gt(a, b);\n', '  #else\n', '    return simde_wasm_i16x8_lt(b, a);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i16x8_gt'
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_gt(a, b);\n', '  #else\n', '    return simde_wasm_i32x4_lt(b, a);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i32x4_gt'
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_gt(a, b);\n', '  #else\n', '    return simde_wasm_i64x2_lt(b, a);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i64x2_gt'
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_gt(a, b);\n', '  #else\n', '    return simde_wasm_u8x16_lt(b, a);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_u8x16_gt'
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_gt(a, b);\n', '  #else\n', '    return simde_wasm_u16x8_lt(b, a);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_u16x8_gt'
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_gt(a, b);\n', '  #else\n', '    return simde_wasm_u32x4_lt(b, a);\n', '  #endif\n', '}\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_u32x4_gt'
}, {
    'name': 'simde_wasm_f32x4_gt',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.i32[i] = (a_.f32[i] > b_.f32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f32 > b_.f32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_gt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcgtq_f32(a_.neon_f32, b_.neon_f32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_cmpgt_ps(a_.sse_m128, b_.sse_m128);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_f32 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(float), vec_cmpgt(a_.altivec_f32, b_.altivec_f32));\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u64 = vcgtq_f64(a_.neon_f64, b_.neon_f64);\n'],
    'name': 'simde_wasm_f64x2_gt',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_f64 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(double), vec_cmpgt(a_.altivec_f64, b_.altivec_f64));\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.i64[i] = (a_.f64[i] > b_.f64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i64 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i64), a_.f64 > b_.f64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_gt(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_cmpgt_pd(a_.sse_m128d, b_.sse_m128d);\n']
}, {
    'name': 'simde_wasm_i8x16_le',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = (a_.i8[i] <= b_.i8[i]) ? ~INT8_C(0) : INT8_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i8 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i8), a_.i8 <= b_.i8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_le(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vcleq_s8(a_.neon_i8, b_.neon_i8);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi8(a_.sse_m128i, _mm_min_epi8(a_.sse_m128i, b_.sse_m128i));\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_i16x8_le',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = (a_.i16[i] <= b_.i16[i]) ? ~INT16_C(0) : INT16_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i16 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i16), a_.i16 <= b_.i16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_le(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vcleq_s16(a_.neon_i16, b_.neon_i16);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi16(a_.sse_m128i, _mm_min_epi16(a_.sse_m128i, b_.sse_m128i));\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_i32x4_le',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = (a_.i32[i] <= b_.i32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.i32 <= b_.i32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_le(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcleq_s32(a_.neon_i32, b_.neon_i32);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi32(a_.sse_m128i, _mm_min_epi32(a_.sse_m128i, b_.sse_m128i));\n'],
    'type': 'simde_v128_t'
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u64 = vcleq_s64(a_.neon_i64, b_.neon_i64);\n'],
    'name': 'simde_wasm_i64x2_le',
    'x86_avx512vl_mapping': ['    #if defined(SIMDE_X86_AVX512VL_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi64(a_.sse_m128i, _mm_min_epi64(a_.sse_m128i, b_.sse_m128i));\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = (a_.i64[i] <= b_.i64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i64 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i64), a_.i64 <= b_.i64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_le(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_u8x16_le',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u8) / sizeof(r_.u8[0])) ; i++) {\n', '        r_.u8[i] = (a_.u8[i] <= b_.u8[i]) ? ~UINT8_C(0) : UINT8_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.u8 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u8), a_.u8 <= b_.u8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_le(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vcleq_u8(a_.neon_u8, b_.neon_u8);\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_u16x8_le',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = (a_.u16[i] <= b_.u16[i]) ? ~UINT16_C(0) : UINT16_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.u16 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u16), a_.u16 <= b_.u16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_le(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vcleq_u16(a_.neon_u16, b_.neon_u16);\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_u32x4_le',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u32) / sizeof(r_.u32[0])) ; i++) {\n', '        r_.u32[i] = (a_.u32[i] <= b_.u32[i]) ? ~UINT32_C(0) : UINT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.u32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u32), a_.u32 <= b_.u32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_le(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcleq_u32(a_.neon_u32, b_.neon_u32);\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_f32x4_le',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.i32[i] = (a_.f32[i] <= b_.f32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f32 <= b_.f32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_le(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcleq_f32(a_.neon_f32, b_.neon_f32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_cmple_ps(a_.sse_m128, b_.sse_m128);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u64 = vcleq_f64(a_.neon_f64, b_.neon_f64);\n'],
    'name': 'simde_wasm_f64x2_le',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.i64[i] = (a_.f64[i] <= b_.f64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f64 <= b_.f64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_le(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_cmple_pd(a_.sse_m128d, b_.sse_m128d);\n']
}, {
    'name': 'simde_wasm_i8x16_ge',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = (a_.i8[i] >= b_.i8[i]) ? ~INT8_C(0) : INT8_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i8 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i8), a_.i8 >= b_.i8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_ge(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vcgeq_s8(a_.neon_i8, b_.neon_i8);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi8(_mm_min_epi8(a_.sse_m128i, b_.sse_m128i), b_.sse_m128i);\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_i16x8_ge',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = (a_.i16[i] >= b_.i16[i]) ? ~INT16_C(0) : INT16_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i16 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i16), a_.i16 >= b_.i16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_ge(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vcgeq_s16(a_.neon_i16, b_.neon_i16);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi16(_mm_min_epi16(a_.sse_m128i, b_.sse_m128i), b_.sse_m128i);\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_i32x4_ge',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = (a_.i32[i] >= b_.i32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.i32 >= b_.i32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_ge(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcgeq_s32(a_.neon_i32, b_.neon_i32);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi32(_mm_min_epi32(a_.sse_m128i, b_.sse_m128i), b_.sse_m128i);\n'],
    'type': 'simde_v128_t'
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u64 = vcgeq_s64(a_.neon_i64, b_.neon_i64);\n'],
    'name': 'simde_wasm_i64x2_ge',
    'x86_avx512vl_mapping': ['    #if defined(SIMDE_X86_AVX512VL_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi64(_mm_min_epi64(a_.sse_m128i, b_.sse_m128i), b_.sse_m128i);\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = (a_.i64[i] >= b_.i64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i64 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i64), a_.i64 >= b_.i64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_ge(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_u8x16_ge',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u8) / sizeof(r_.u8[0])) ; i++) {\n', '        r_.u8[i] = (a_.u8[i] >= b_.u8[i]) ? ~UINT8_C(0) : UINT8_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.u8 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u8), a_.u8 >= b_.u8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_ge(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vcgeq_u8(a_.neon_u8, b_.neon_u8);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi8(_mm_min_epu8(a_.sse_m128i, b_.sse_m128i), b_.sse_m128i);\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_u16x8_ge',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = (a_.u16[i] >= b_.u16[i]) ? ~UINT16_C(0) : UINT16_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.u16 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u16), a_.u16 >= b_.u16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_ge(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vcgeq_u16(a_.neon_u16, b_.neon_u16);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi16(_mm_min_epu16(a_.sse_m128i, b_.sse_m128i), b_.sse_m128i);\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_u32x4_ge',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u32) / sizeof(r_.u32[0])) ; i++) {\n', '        r_.u32[i] = (a_.u32[i] >= b_.u32[i]) ? ~UINT32_C(0) : UINT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.u32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u32), a_.u32 >= b_.u32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_ge(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcgeq_u32(a_.neon_u32, b_.neon_u32);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cmpeq_epi32(_mm_min_epu32(a_.sse_m128i, b_.sse_m128i), b_.sse_m128i);\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_f32x4_ge',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.i32[i] = (a_.f32[i] >= b_.f32[i]) ? ~INT32_C(0) : INT32_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f32 >= b_.f32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_ge(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcgeq_f32(a_.neon_f32, b_.neon_f32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_cmpge_ps(a_.sse_m128, b_.sse_m128);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u64 = vcgeq_f64(a_.neon_f64, b_.neon_f64);\n'],
    'name': 'simde_wasm_f64x2_ge',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.i64[i] = (a_.f64[i] >= b_.f64[i]) ? ~INT64_C(0) : INT64_C(0);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f64 >= b_.f64);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_ge(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_cmpge_pd(a_.sse_m128d, b_.sse_m128d);\n']
}, {
    'name': 'simde_wasm_v128_not',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32f) / sizeof(r_.i32f[0])) ; i++) {\n', '        r_.i32f[i] = ~(a_.i32f[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32f = ~a_.i32f;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_not(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vmvnq_s32(a_.neon_i32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_xor_si128(a_.sse_m128i, _mm_set1_epi32(~INT32_C(0)));\n']
}, {
    'name': 'simde_wasm_v128_and',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32f) / sizeof(r_.i32f[0])) ; i++) {\n', '        r_.i32f[i] = a_.i32f[i] & b_.i32f[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32f = a_.i32f & b_.i32f;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_and(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vandq_s32(a_.neon_i32, b_.neon_i32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_and_si128(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_v128_or',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32f) / sizeof(r_.i32f[0])) ; i++) {\n', '        r_.i32f[i] = a_.i32f[i] | b_.i32f[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32f = a_.i32f | b_.i32f;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_or(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vorrq_s32(a_.neon_i32, b_.neon_i32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_or_si128(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_v128_xor',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32f) / sizeof(r_.i32f[0])) ; i++) {\n', '        r_.i32f[i] = a_.i32f[i] ^ b_.i32f[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32f = a_.i32f ^ b_.i32f;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_xor(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = veorq_s32(a_.neon_i32, b_.neon_i32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_xor_si128(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_v128_andnot',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32f) / sizeof(r_.i32f[0])) ; i++) {\n', '        r_.i32f[i] = a_.i32f[i] & ~b_.i32f[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32f = a_.i32f & ~b_.i32f;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_andnot(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vbicq_s32(a_.neon_i32, b_.neon_i32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_andnot_si128(b_.sse_m128i, a_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_v128_bitselect',
    'x86_avx512vl_mapping': ['    #if defined(SIMDE_X86_AVX512VL_NATIVE)\n', '      r_.sse_m128i = _mm_ternarylogic_epi32(mask_.sse_m128i, a_.sse_m128i, b_.sse_m128i, 0xca);\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32f) / sizeof(r_.i32f[0])) ; i++) {\n', '        r_.i32f[i] = (a_.i32f[i] & mask_.i32f[i]) | (b_.i32f[i] & ~mask_.i32f[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32f = (a_.i32f & mask_.i32f) | (b_.i32f & ~mask_.i32f);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_bitselect(a, b, mask);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      mask_ = simde_v128_to_private(mask),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vbslq_s32(mask_.neon_u32, a_.neon_i32, b_.neon_i32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_or_si128(\n', '          _mm_and_si128   (mask_.sse_m128i, a_.sse_m128i),\n', '          _mm_andnot_si128(mask_.sse_m128i, b_.sse_m128i));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_i32 = vec_sel(b_.altivec_i32, a_.altivec_i32, mask_.altivec_u32);\n']
}, {
    'arm_neon_a64v8_mapping': ['      #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '        r = vaddvq_u16(x);\n', '      #else\n', '        uint64x2_t t64 = vpaddlq_u32(vpaddlq_u16(x));\n', '        r =\n', '          HEDLEY_STATIC_CAST(uint32_t, vgetq_lane_u64(t64, 0)) +\n', '          HEDLEY_STATIC_CAST(uint32_t, vgetq_lane_u64(t64, 1));\n', '      #endif\n'],
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE) && defined(SIMDE_BUG_CLANG_50932)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) idx = { 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 0 };\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) res = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned char), vec_bperm(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned __int128), a_.altivec_u64), idx));\n', '      r = HEDLEY_STATIC_CAST(uint32_t, vec_extract(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), res), 2));\n', '    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) idx = { 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 0 };\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) res = vec_bperm(a_.altivec_u8, idx);\n', '      r = HEDLEY_STATIC_CAST(uint32_t, vec_extract(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), res), 2));\n', '    #else\n'],
    'name': 'simde_wasm_i8x16_bitmask',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE_REDUCTION(|:r)\n', '      for (size_t i = 0 ; i < (sizeof(a_.i8) / sizeof(a_.i8[0])) ; i++) {\n', '        r |= HEDLEY_STATIC_CAST(uint32_t, (a_.i8[i] < 0) << i);\n', '      }\n', '    #endif\n', '\n', '    return r;\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_bitmask(a);\n', '  #else\n', '    simde_v128_private a_ = simde_v128_to_private(a);\n', '    uint32_t r = 0;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      /* https://github.com/WebAssembly/simd/pull/201#issue-380682845 */\n', '      static const uint8_t md[16] = {\n', '        1 << 0, 1 << 1, 1 << 2, 1 << 3,\n', '        1 << 4, 1 << 5, 1 << 6, 1 << 7,\n', '        1 << 0, 1 << 1, 1 << 2, 1 << 3,\n', '        1 << 4, 1 << 5, 1 << 6, 1 << 7,\n', '      };\n', '\n', '      /* Extend sign bit over entire lane */\n', '      uint8x16_t extended = vreinterpretq_u8_s8(vshrq_n_s8(a_.neon_i8, 7));\n', "      /* Clear all but the bit we're interested in. */\n", '      uint8x16_t masked = vandq_u8(vld1q_u8(md), extended);\n', '      /* Alternate bytes from low half and high half */\n', '      uint8x8x2_t tmp = vzip_u8(vget_low_u8(masked), vget_high_u8(masked));\n', '      uint16x8_t x = vreinterpretq_u16_u8(vcombine_u8(tmp.val[0], tmp.val[1]));\n'],
    'type': 'uint32_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r = HEDLEY_STATIC_CAST(uint32_t, _mm_movemask_epi8(a_.sse_m128i));\n']
}, {
    'arm_neon_a64v8_mapping': ['      #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '        r = vaddvq_u16(masked);\n', '      #else\n', '        uint64x2_t t64 = vpaddlq_u32(vpaddlq_u16(masked));\n', '        r =\n', '          HEDLEY_STATIC_CAST(uint32_t, vgetq_lane_u64(t64, 0)) +\n', '          HEDLEY_STATIC_CAST(uint32_t, vgetq_lane_u64(t64, 1));\n', '      #endif\n'],
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE) && defined(SIMDE_BUG_CLANG_50932)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) idx = { 112, 96, 80, 64, 48, 32, 16, 0, 128, 128, 128, 128, 128, 128, 128, 128 };\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) res = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned char), vec_bperm(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned __int128), a_.altivec_u64), idx));\n', '      r = HEDLEY_STATIC_CAST(uint32_t, vec_extract(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), res), 2));\n', '    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) idx = { 112, 96, 80, 64, 48, 32, 16, 0, 128, 128, 128, 128, 128, 128, 128, 128 };\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) res = vec_bperm(a_.altivec_u8, idx);\n', '      r = HEDLEY_STATIC_CAST(uint32_t, vec_extract(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), res), 2));\n', '    #else\n'],
    'name': 'simde_wasm_i16x8_bitmask',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE_REDUCTION(|:r)\n', '      for (size_t i = 0 ; i < (sizeof(a_.i16) / sizeof(a_.i16[0])) ; i++) {\n', '        r |= HEDLEY_STATIC_CAST(uint32_t, (a_.i16[i] < 0) << i);\n', '      }\n', '    #endif\n', '\n', '    return r;\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_bitmask(a);\n', '  #else\n', '    simde_v128_private a_ = simde_v128_to_private(a);\n', '    uint32_t r = 0;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      static const uint16_t md[8] = {\n', '        1 << 0, 1 << 1, 1 << 2, 1 << 3,\n', '        1 << 4, 1 << 5, 1 << 6, 1 << 7,\n', '      };\n', '\n', '      uint16x8_t extended = vreinterpretq_u16_s16(vshrq_n_s16(a_.neon_i16, 15));\n', '      uint16x8_t masked = vandq_u16(vld1q_u16(md), extended);\n'],
    'type': 'uint32_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r = HEDLEY_STATIC_CAST(uint32_t, _mm_movemask_epi8(_mm_packs_epi16(a_.sse_m128i, _mm_setzero_si128())));\n']
}, {
    'arm_neon_a64v8_mapping': ['      #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '        r = HEDLEY_STATIC_CAST(uint32_t, vaddvq_u32(masked));\n', '      #else\n', '        uint64x2_t t64 = vpaddlq_u32(masked);\n', '        r =\n', '          HEDLEY_STATIC_CAST(uint32_t, vgetq_lane_u64(t64, 0)) +\n', '          HEDLEY_STATIC_CAST(uint32_t, vgetq_lane_u64(t64, 1));\n', '      #endif\n'],
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE) && defined(SIMDE_BUG_CLANG_50932)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) idx = { 96, 64, 32, 0, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128 };\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) res = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned char), vec_bperm(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned __int128), a_.altivec_u64), idx));\n', '      r = HEDLEY_STATIC_CAST(uint32_t, vec_extract(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), res), 2));\n', '    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) idx = { 96, 64, 32, 0, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128 };\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) res = vec_bperm(a_.altivec_u8, idx);\n', '      r = HEDLEY_STATIC_CAST(uint32_t, vec_extract(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), res), 2));\n', '    #else\n'],
    'name': 'simde_wasm_i32x4_bitmask',
    'x86_sse_mapping': ['    #if defined(SIMDE_X86_SSE_NATIVE)\n', '      r = HEDLEY_STATIC_CAST(uint32_t, _mm_movemask_ps(a_.sse_m128));\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE_REDUCTION(|:r)\n', '      for (size_t i = 0 ; i < (sizeof(a_.i32) / sizeof(a_.i32[0])) ; i++) {\n', '        r |= HEDLEY_STATIC_CAST(uint32_t, (a_.i32[i] < 0) << i);\n', '      }\n', '    #endif\n', '\n', '    return r;\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_bitmask(a);\n', '  #else\n', '    simde_v128_private a_ = simde_v128_to_private(a);\n', '    uint32_t r = 0;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      static const uint32_t md[4] = {\n', '        1 << 0, 1 << 1, 1 << 2, 1 << 3\n', '      };\n', '\n', '      uint32x4_t extended = vreinterpretq_u32_s32(vshrq_n_s32(a_.neon_i32, 31));\n', '      uint32x4_t masked = vandq_u32(vld1q_u32(md), extended);\n'],
    'type': 'uint32_t'
}, {
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE) && defined(SIMDE_BUG_CLANG_50932)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) idx = { 64, 0, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128 };\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) res = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned char), vec_bperm(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned __int128), a_.altivec_u64), idx));\n', '      r = HEDLEY_STATIC_CAST(uint32_t, vec_extract(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), res), 2));\n', '    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) idx = { 64, 0, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128 };\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) res = vec_bperm(a_.altivec_u8, idx);\n', '      r = HEDLEY_STATIC_CAST(uint32_t, vec_extract(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed int), res), 2));\n', '    #else\n'],
    'name': 'simde_wasm_i64x2_bitmask',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE_REDUCTION(|:r)\n', '      for (size_t i = 0 ; i < (sizeof(a_.i64) / sizeof(a_.i64[0])) ; i++) {\n', '        r |= HEDLEY_STATIC_CAST(uint32_t, (a_.i64[i] < 0) << i);\n', '      }\n', '    #endif\n', '\n', '    return r;\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_bitmask(a);\n', '  #else\n', '    simde_v128_private a_ = simde_v128_to_private(a);\n', '    uint32_t r = 0;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      HEDLEY_DIAGNOSTIC_PUSH\n', '      SIMDE_DIAGNOSTIC_DISABLE_VECTOR_CONVERSION_\n', '      uint64x2_t shifted = vshrq_n_u64(a_.neon_u64, 63);\n', '      r =\n', '        HEDLEY_STATIC_CAST(uint32_t, vgetq_lane_u64(shifted, 0)) +\n', '        (HEDLEY_STATIC_CAST(uint32_t, vgetq_lane_u64(shifted, 1)) << 1);\n', '      HEDLEY_DIAGNOSTIC_POP\n'],
    'type': 'uint32_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r = HEDLEY_STATIC_CAST(uint32_t, _mm_movemask_pd(a_.sse_m128d));\n']
}, {
    'name': 'simde_wasm_i8x16_abs',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = (a_.i8[i] < INT8_C(0)) ? -a_.i8[i] : a_.i8[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_abs(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vabsq_s8(a_.neon_i8);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_abs_epi8(a_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = vec_abs(a_.altivec_i8);\n', '    #elif defined(SIMDE_VECTOR_SCALAR)\n', '      __typeof__(r_.i8) mask = HEDLEY_REINTERPRET_CAST(__typeof__(mask), a_.i8 < 0);\n', '      r_.i8 = (-a_.i8 & mask) | (a_.i8 & ~mask);\n', '    #else\n']
}, {
    'name': 'simde_wasm_i16x8_abs',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = (a_.i16[i] < INT8_C(0)) ? -a_.i16[i] : a_.i16[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_abs(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vabsq_s16(a_.neon_i16);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_abs_epi16(a_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = vec_abs(a_.altivec_i16);\n', '    #else\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_i32 = vabsq_s32(a_.neon_i32);\n'],
    'name': 'simde_wasm_i32x4_abs',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = (a_.i32[i] < INT32_C(0)) ? -a_.i32[i] : a_.i32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      __typeof__(r_.i32) z = { 0, };\n', '      __typeof__(r_.i32) m = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.i32 < z);\n', '      r_.i32 = (-a_.i32 & m) | (a_.i32 & ~m);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_abs(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_abs_epi32(a_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 = vec_abs(a_.altivec_i32);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_i64 = vabsq_s64(a_.neon_i64);\n'],
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_i64 = vec_abs(a_.altivec_i64);\n'],
    'name': 'simde_wasm_i64x2_abs',
    'x86_avx512vl_mapping': ['    #if defined(SIMDE_X86_AVX512VL_NATIVE)\n', '      r_.sse_m128i = _mm_abs_epi64(a_.sse_m128i);\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = (a_.i64[i] < INT64_C(0)) ? -a_.i64[i] : a_.i64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      __typeof__(r_.i64) z = { 0, };\n', '      __typeof__(r_.i64) m = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i64), a_.i64 < z);\n', '      r_.i64 = (-a_.i64 & m) | (a_.i64 & ~m);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_abs(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_f32x4_abs',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = (a_.f32[i] < SIMDE_FLOAT32_C(0.0)) ? -a_.f32[i] : a_.f32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      int32_t SIMDE_VECTOR(16) m = HEDLEY_REINTERPRET_CAST(__typeof__(m), a_.f32 < SIMDE_FLOAT32_C(0.0));\n', '      r_.f32 =\n', '        HEDLEY_REINTERPRET_CAST(\n', '          __typeof__(r_.f32),\n', '          (\n', '            (HEDLEY_REINTERPRET_CAST(__typeof__(m), -a_.f32) & m) |\n', '            (HEDLEY_REINTERPRET_CAST(__typeof__(m), a_.f32) & ~m)\n', '          )\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_abs(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_f32 = vabsq_f32(a_.neon_f32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_andnot_si128(_mm_set1_epi32(HEDLEY_STATIC_CAST(int32_t, UINT32_C(1) << 31)), a_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_f32 = vec_abs(a_.altivec_f32);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f64 = vabsq_f64(a_.neon_f64);\n'],
    'name': 'simde_wasm_f64x2_abs',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_f64 = vec_abs(a_.altivec_f64);\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = (a_.f64[i] < SIMDE_FLOAT64_C(0.0)) ? -a_.f64[i] : a_.f64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      int64_t SIMDE_VECTOR(16) m = HEDLEY_REINTERPRET_CAST(__typeof__(m), a_.f64 < SIMDE_FLOAT64_C(0.0));\n', '      r_.f64 =\n', '        HEDLEY_REINTERPRET_CAST(\n', '          __typeof__(r_.f64),\n', '          (\n', '            (HEDLEY_REINTERPRET_CAST(__typeof__(m), -a_.f64) &  m) |\n', '            (HEDLEY_REINTERPRET_CAST(__typeof__(m),  a_.f64) & ~m)\n', '          )\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_abs(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_andnot_si128(_mm_set1_epi64x(HEDLEY_STATIC_CAST(int64_t, UINT64_C(1) << 63)), a_.sse_m128i);\n']
}, {
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE) && (!defined(HEDLEY_GCC_VERSION) || HEDLEY_GCC_VERSION_CHECK(8,1,0))\n', '      r_.altivec_i8 = vec_neg(a_.altivec_i8);\n'],
    'name': 'simde_wasm_i8x16_neg',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = -a_.i8[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i8 = -a_.i8;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_neg(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vnegq_s8(a_.neon_i8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_sub_epi8(_mm_setzero_si128(), a_.sse_m128i);\n']
}, {
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_i16 = vec_neg(a_.altivec_i16);\n'],
    'name': 'simde_wasm_i16x8_neg',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = -a_.i16[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i16 = -a_.i16;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_neg(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vnegq_s16(a_.neon_i16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_sub_epi16(_mm_setzero_si128(), a_.sse_m128i);\n']
}, {
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_i32 = vec_neg(a_.altivec_i32);\n'],
    'name': 'simde_wasm_i32x4_neg',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = -a_.i32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = -a_.i32;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_neg(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vnegq_s32(a_.neon_i32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_sub_epi32(_mm_setzero_si128(), a_.sse_m128i);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_i64 = vnegq_s64(a_.neon_i64);\n'],
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_i64 = vec_neg(a_.altivec_i64);\n'],
    'name': 'simde_wasm_i64x2_neg',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = -a_.i64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i64 = -a_.i64;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_neg(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_sub_epi64(_mm_setzero_si128(), a_.sse_m128i);\n']
}, {
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_f32 = vec_neg(a_.altivec_f32);\n'],
    'name': 'simde_wasm_f32x4_neg',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = -a_.f32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.f32 = -a_.f32;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_neg(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_f32 = vnegq_f32(a_.neon_f32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_xor_si128(_mm_set1_epi32(HEDLEY_STATIC_CAST(int32_t, UINT32_C(1) << 31)), a_.sse_m128i);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f64 = vnegq_f64(a_.neon_f64);\n'],
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_f64 = vec_neg(a_.altivec_f64);\n'],
    'name': 'simde_wasm_f64x2_neg',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = -a_.f64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.f64 = -a_.f64;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_neg(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_xor_si128(_mm_set1_epi64x(HEDLEY_STATIC_CAST(int64_t, UINT64_C(1) << 63)), a_.sse_m128i);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r = !!vmaxvq_u32(a_.neon_u32);\n'],
    'name': 'simde_wasm_v128_any_true',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE_REDUCTION(|:ri)\n', '      for (size_t i = 0 ; i < (sizeof(a_.i32f) / sizeof(a_.i32f[0])) ; i++) {\n', '        ri |= (a_.i32f[i]);\n', '      }\n', '      r = !!ri;\n', '    #endif\n', '\n', '    return r;\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_any_true(a);\n', '  #else\n', '    simde_v128_private a_ = simde_v128_to_private(a);\n', '    simde_bool r = 0;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      uint32x2_t tmp = vpmax_u32(vget_low_u32(a_.u32), vget_high_u32(a_.u32));\n', '      r  = vget_lane_u32(tmp, 0);\n', '      r |= vget_lane_u32(tmp, 1);\n', '      r = !!r;\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r = !_mm_test_all_zeros(a_.sse_m128i, _mm_set1_epi32(~INT32_C(0)));\n'],
    'type': 'simde_bool',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r = _mm_movemask_epi8(_mm_cmpeq_epi8(a_.sse_m128i, _mm_setzero_si128())) != 0xffff;\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r = HEDLEY_STATIC_CAST(simde_bool, vec_any_ne(a_.altivec_i32, vec_splats(0)));\n', '    #else\n', '      int_fast32_t ri = 0;\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      return !vmaxvq_u8(vceqzq_u8(a_.neon_u8));\n'],
    'name': 'simde_wasm_i8x16_all_true',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE_REDUCTION(&:r)\n', '      for (size_t i = 0 ; i < (sizeof(a_.i8) / sizeof(a_.i8[0])) ; i++) {\n', '        r &= !!(a_.i8[i]);\n', '      }\n', '\n', '      return r;\n', '    #endif\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_all_true(a);\n', '  #else\n', '    simde_v128_private a_ = simde_v128_to_private(a);\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      uint8x16_t zeroes = vdupq_n_u8(0);\n', '      uint8x16_t false_set = vceqq_u8(a_.neon_u8, vdupq_n_u8(0));\n', '      uint32x4_t d_all_true = vceqq_u32(vreinterpretq_u32_u8(false_set), vreinterpretq_u32_u8(zeroes));\n', '      uint32x2_t q_all_true = vpmin_u32(vget_low_u32(d_all_true), vget_high_u32(d_all_true));\n', '\n', '      return !!(\n', '        vget_lane_u32(q_all_true, 0) &\n', '        vget_lane_u32(q_all_true, 1));\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      return _mm_test_all_zeros(_mm_cmpeq_epi8(a_.sse_m128i, _mm_set1_epi8(INT8_C(0))), _mm_set1_epi8(~INT8_C(0)));\n'],
    'type': 'simde_bool',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      return _mm_movemask_epi8(_mm_cmpeq_epi8(a_.sse_m128i, _mm_setzero_si128())) == 0;\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      return HEDLEY_STATIC_CAST(simde_bool, vec_all_ne(a_.altivec_i8, vec_splats(HEDLEY_STATIC_CAST(signed char, 0))));\n', '    #else\n', '      int8_t r = !INT8_C(0);\n', '\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      return !vmaxvq_u16(vceqzq_u16(a_.neon_u16));\n'],
    'name': 'simde_wasm_i16x8_all_true',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE_REDUCTION(&:r)\n', '      for (size_t i = 0 ; i < (sizeof(a_.i16) / sizeof(a_.i16[0])) ; i++) {\n', '        r &= !!(a_.i16[i]);\n', '      }\n', '\n', '      return r;\n', '    #endif\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_all_true(a);\n', '  #else\n', '    simde_v128_private a_ = simde_v128_to_private(a);\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      uint16x8_t zeroes = vdupq_n_u16(0);\n', '      uint16x8_t false_set = vceqq_u16(a_.neon_u16, vdupq_n_u16(0));\n', '      uint32x4_t d_all_true = vceqq_u32(vreinterpretq_u32_u16(false_set), vreinterpretq_u32_u16(zeroes));\n', '      uint32x2_t q_all_true = vpmin_u32(vget_low_u32(d_all_true), vget_high_u32(d_all_true));\n', '\n', '      return !!(\n', '        vget_lane_u32(q_all_true, 0) &\n', '        vget_lane_u32(q_all_true, 1));\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      return _mm_test_all_zeros(_mm_cmpeq_epi16(a_.sse_m128i, _mm_setzero_si128()), _mm_set1_epi16(~INT16_C(0)));\n'],
    'type': 'simde_bool',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      return _mm_movemask_epi8(_mm_cmpeq_epi16(a_.sse_m128i, _mm_setzero_si128())) == 0;\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      return HEDLEY_STATIC_CAST(simde_bool, vec_all_ne(a_.altivec_i16, vec_splats(HEDLEY_STATIC_CAST(signed short, 0))));\n', '    #else\n', '      int16_t r = !INT16_C(0);\n', '\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      return !vmaxvq_u32(vceqzq_u32(a_.neon_u32));\n'],
    'name': 'simde_wasm_i32x4_all_true',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE_REDUCTION(&:r)\n', '      for (size_t i = 0 ; i < (sizeof(a_.i32) / sizeof(a_.i32[0])) ; i++) {\n', '        r &= !!(a_.i32[i]);\n', '      }\n', '\n', '      return r;\n', '    #endif\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_all_true(a);\n', '  #else\n', '    simde_v128_private a_ = simde_v128_to_private(a);\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      uint32x4_t d_all_true = vmvnq_u32(vceqq_u32(a_.neon_u32, vdupq_n_u32(0)));\n', '      uint32x2_t q_all_true = vpmin_u32(vget_low_u32(d_all_true), vget_high_u32(d_all_true));\n', '\n', '      return !!(\n', '        vget_lane_u32(q_all_true, 0) &\n', '        vget_lane_u32(q_all_true, 1));\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      return _mm_test_all_zeros(_mm_cmpeq_epi32(a_.sse_m128i, _mm_setzero_si128()), _mm_set1_epi32(~INT32_C(0)));\n'],
    'type': 'simde_bool',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      return _mm_movemask_ps(_mm_castsi128_ps(_mm_cmpeq_epi32(a_.sse_m128i, _mm_setzero_si128()))) == 0;\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      return HEDLEY_STATIC_CAST(simde_bool, vec_all_ne(a_.altivec_i32, vec_splats(HEDLEY_STATIC_CAST(signed int, 0))));\n', '    #else\n', '      int32_t r = !INT32_C(0);\n', '\n']
}, {
    'name': 'simde_wasm_i64x2_all_true',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      return HEDLEY_STATIC_CAST(simde_bool, vec_all_ne(a_.altivec_i64, HEDLEY_REINTERPRET_CAST(__typeof__(a_.altivec_i64), vec_splats(0))));\n', '    #else\n', '      int64_t r = !INT32_C(0);\n', '\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE_REDUCTION(&:r)\n', '      for (size_t i = 0 ; i < (sizeof(a_.i64) / sizeof(a_.i64[0])) ; i++) {\n', '        r &= !!(a_.i64[i]);\n', '      }\n', '\n', '      return r;\n', '    #endif\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE) && defined(__wasm_unimplemented_simd128__)\n', '    return wasm_i64x2_all_true(a);\n', '  #else\n', '    simde_v128_private a_ = simde_v128_to_private(a);\n', '\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      return _mm_test_all_zeros(_mm_cmpeq_epi64(a_.sse_m128i, _mm_setzero_si128()), _mm_set1_epi32(~INT32_C(0)));\n'],
    'type': 'simde_bool',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      return _mm_movemask_pd(_mm_cmpeq_pd(a_.sse_m128d, _mm_setzero_pd())) == 0;\n']
}, {
    'name': 'simde_wasm_i8x16_shl',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = HEDLEY_STATIC_CAST(int8_t, a_.i8[i] << (count & 7));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.i8 = a_.i8 << (count & 7);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_shl(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vshlq_s8(a_.neon_i8, vdupq_n_s8(HEDLEY_STATIC_CAST(int8_t, count)));\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = vec_sl(a_.altivec_i8, vec_splats(HEDLEY_STATIC_CAST(unsigned char, count)));\n']
}, {
    'name': 'simde_wasm_i16x8_shl',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = HEDLEY_STATIC_CAST(int16_t, a_.i16[i] << (count & 15));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.i16 = a_.i16 << (count & 15);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_shl(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vshlq_s16(a_.neon_i16, vdupq_n_s16(HEDLEY_STATIC_CAST(int16_t, count)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      return _mm_sll_epi16(a_.sse_m128i, _mm_cvtsi32_si128(count & 15));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = vec_sl(a_.altivec_i16, vec_splats(HEDLEY_STATIC_CAST(unsigned short, count)));\n']
}, {
    'name': 'simde_wasm_i32x4_shl',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, a_.i32[i] << (count & 31));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.i32 = a_.i32 << (count & 31);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_shl(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vshlq_s32(a_.neon_i32, vdupq_n_s32(HEDLEY_STATIC_CAST(int32_t, count)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      return _mm_sll_epi32(a_.sse_m128i, _mm_cvtsi32_si128(count & 31));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 = vec_sl(a_.altivec_i32, vec_splats(HEDLEY_STATIC_CAST(unsigned int, count)));\n']
}, {
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_i64 = vec_sl(a_.altivec_i64, vec_splats(HEDLEY_STATIC_CAST(unsigned long long, count)));\n'],
    'name': 'simde_wasm_i64x2_shl',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = HEDLEY_STATIC_CAST(int64_t, a_.i64[i] << (count & 63));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.i64 = a_.i64 << (count & 63);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_shl(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i64 = vshlq_s64(a_.neon_i64, vdupq_n_s64(HEDLEY_STATIC_CAST(int64_t, count)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      return _mm_sll_epi64(a_.sse_m128i, _mm_cvtsi32_si128(count & 63));\n']
}, {
    'name': 'simde_wasm_i8x16_shr',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = HEDLEY_STATIC_CAST(int8_t, a_.i8[i] >> (count & 7));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.i8 = a_.i8 >> (count & 7);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_shr(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vshlq_s8(a_.neon_i8, vdupq_n_s8(HEDLEY_STATIC_CAST(int8_t, -count)));\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = vec_sra(a_.altivec_i8, vec_splats(HEDLEY_STATIC_CAST(unsigned char, count)));\n']
}, {
    'name': 'simde_wasm_i16x8_shr',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = HEDLEY_STATIC_CAST(int16_t, a_.i16[i] >> (count & 15));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.i16 = a_.i16 >> (count & 15);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_shr(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vshlq_s16(a_.neon_i16, vdupq_n_s16(HEDLEY_STATIC_CAST(int16_t, -count)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      return _mm_sra_epi16(a_.sse_m128i, _mm_cvtsi32_si128(count & 15));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = vec_sra(a_.altivec_i16, vec_splats(HEDLEY_STATIC_CAST(unsigned short, count)));\n']
}, {
    'name': 'simde_wasm_i32x4_shr',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, a_.i32[i] >> (count & 31));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.i32 = a_.i32 >> (count & 31);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_shr(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vshlq_s32(a_.neon_i32, vdupq_n_s32(HEDLEY_STATIC_CAST(int32_t, -count)));\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      return _mm_sra_epi32(a_.sse_m128i, _mm_cvtsi32_si128(count & 31));\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 = vec_sra(a_.altivec_i32, vec_splats(HEDLEY_STATIC_CAST(unsigned int, count)));\n']
}, {
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_i64 = vec_sra(a_.altivec_i64, vec_splats(HEDLEY_STATIC_CAST(unsigned long long, count)));\n'],
    'name': 'simde_wasm_i64x2_shr',
    'x86_avx512vl_mapping': ['    #if defined(SIMDE_X86_AVX512VL_NATIVE)\n', '      return _mm_sra_epi64(a_.sse_m128i, _mm_cvtsi32_si128(count & 63));\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = HEDLEY_STATIC_CAST(int64_t, a_.i64[i] >> (count & 63));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.i64 = a_.i64 >> (count & 63);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_shr(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i64 = vshlq_s64(a_.neon_i64, vdupq_n_s64(HEDLEY_STATIC_CAST(int64_t, -count)));\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_u8x16_shr',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u8) / sizeof(r_.u8[0])) ; i++) {\n', '        r_.u8[i] = HEDLEY_STATIC_CAST(uint8_t, a_.u8[i] >> (count & 7));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.u8 = a_.u8 >> (count & 7);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_shr(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vshlq_u8(a_.neon_u8, vdupq_n_s8(HEDLEY_STATIC_CAST(int8_t, -count)));\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u8 = vec_sr(a_.altivec_u8, vec_splats(HEDLEY_STATIC_CAST(unsigned char, count)));\n']
}, {
    'name': 'simde_wasm_u16x8_shr',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = HEDLEY_STATIC_CAST(uint16_t, a_.u16[i] >> (count & 15));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.u16 = a_.u16 >> (count & 15);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_shr(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vshlq_u16(a_.neon_u16, vdupq_n_s16(HEDLEY_STATIC_CAST(int16_t, -count)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      return _mm_srl_epi16(a_.sse_m128i, _mm_cvtsi32_si128(count & 15));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = vec_sra(a_.altivec_i16, vec_splats(HEDLEY_STATIC_CAST(unsigned short, count)));\n']
}, {
    'name': 'simde_wasm_u32x4_shr',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u32) / sizeof(r_.u32[0])) ; i++) {\n', '        r_.u32[i] = HEDLEY_STATIC_CAST(uint32_t, a_.u32[i] >> (count & 31));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.u32 = a_.u32 >> (count & 31);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_shr(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vshlq_u32(a_.neon_u32, vdupq_n_s32(HEDLEY_STATIC_CAST(int32_t, -count)));\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      return _mm_srl_epi32(a_.sse_m128i, _mm_cvtsi32_si128(count & 31));\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 = vec_sra(a_.altivec_i32, vec_splats(HEDLEY_STATIC_CAST(unsigned int, count)));\n']
}, {
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_i64 = vec_sra(a_.altivec_i64, vec_splats(HEDLEY_STATIC_CAST(unsigned long long, count)));\n'],
    'name': 'simde_wasm_u64x2_shr',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u64) / sizeof(r_.u64[0])) ; i++) {\n', '        r_.u64[i] = HEDLEY_STATIC_CAST(uint64_t, a_.u64[i] >> (count & 63));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT) && defined(SIMDE_VECTOR_SCALAR)\n', '      r_.u64 = a_.u64 >> (count & 63);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u64x2_shr(a, count);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u64 = vshlq_u64(a_.neon_u64, vdupq_n_s64(HEDLEY_STATIC_CAST(int64_t, -count)));\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      return _mm_srl_epi64(a_.sse_m128i, _mm_cvtsi32_si128(count & 63));\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_i8x16_add',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = a_.i8[i] + b_.i8[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i8 = a_.i8 + b_.i8;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_add(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_add_epi8(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_i16x8_add',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = a_.i16[i] + b_.i16[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i16 = a_.i16 + b_.i16;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_add(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_add_epi16(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_i32x4_add',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = a_.i32[i] + b_.i32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = a_.i32 + b_.i32;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_add(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_add_epi32(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_i64x2_add',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = a_.i64[i] + b_.i64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i64 = a_.i64 + b_.i64;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_add(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_add_epi64(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_f32x4_add',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = a_.f32[i] + b_.f32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.f32 = a_.f32 + b_.f32;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_add(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_add_ps(a_.sse_m128, b_.sse_m128);\n']
}, {
    'name': 'simde_wasm_f64x2_add',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = a_.f64[i] + b_.f64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.f64 = a_.f64 + b_.f64;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_add(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_add_pd(a_.sse_m128d, b_.sse_m128d);\n']
}, {
    'name': 'simde_wasm_i8x16_sub',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = a_.i8[i] - b_.i8[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i8 = a_.i8 - b_.i8;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_sub(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_sub_epi8(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_i16x8_sub',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = a_.i16[i] - b_.i16[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i16 = a_.i16 - b_.i16;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_sub(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_sub_epi16(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_i32x4_sub',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = a_.i32[i] - b_.i32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = a_.i32 - b_.i32;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_sub(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_sub_epi32(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_i64x2_sub',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = a_.i64[i] - b_.i64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i64 = a_.i64 - b_.i64;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_sub(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_sub_epi64(a_.sse_m128i, b_.sse_m128i);\n']
}, {
    'name': 'simde_wasm_f32x4_sub',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = a_.f32[i] - b_.f32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.f32 = a_.f32 - b_.f32;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_sub(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_sub_ps(a_.sse_m128, b_.sse_m128);\n']
}, {
    'name': 'simde_wasm_f64x2_sub',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = a_.f64[i] - b_.f64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.f64 = a_.f64 - b_.f64;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_sub(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_sub_pd(a_.sse_m128d, b_.sse_m128d);\n']
}, {
    'name': 'simde_wasm_i16x8_mul',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = a_.i16[i] * b_.i16[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i16 = a_.i16 * b_.i16;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_mul(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vmulq_s16(a_.neon_i16, b_.neon_i16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_mullo_epi16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 =\n', '        vec_pack(\n', '          vec_mule(a_.altivec_i16, b_.altivec_i16),\n', '          vec_mulo(a_.altivec_i16, b_.altivec_i16)\n', '        );\n']
}, {
    'name': 'simde_wasm_i32x4_mul',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = a_.i32[i] * b_.i32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i32 = a_.i32 * b_.i32;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_mul(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_mullo_epi32(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_i64x2_mul',
    'x86_avx512vl_mapping': ['    #if defined(SIMDE_X86_AVX512VL_NATIVE) && defined(SIMDE_X86_AVX512DQ_NATIVE)\n', '      r_.sse_m128i = _mm_mullo_epi64(a_.sse_m128i, b_.sse_m128i);\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = a_.i64[i] * b_.i64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.i64 = a_.i64 * b_.i64;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_mul(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t'
}, {
    'name': 'simde_wasm_f32x4_mul',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = a_.f32[i] * b_.f32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.f32 = a_.f32 * b_.f32;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_mul(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_mul_ps(a_.sse_m128, b_.sse_m128);\n']
}, {
    'name': 'simde_wasm_f64x2_mul',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = a_.f64[i] * b_.f64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.f64 = a_.f64 * b_.f64;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_mul(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_mul_pd(a_.sse_m128d, b_.sse_m128d);\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        int32_t tmp = HEDLEY_STATIC_CAST(int32_t, a_.i16[i]) * HEDLEY_STATIC_CAST(int32_t, b_.i16[i]);\n', '        tmp += UINT32_C(0x4000);\n', '        tmp >>= 15;\n', '        r_.i16[i] = (tmp < INT16_MIN) ? INT16_MIN : ((tmp > INT16_MAX) ? (INT16_MAX) : HEDLEY_STATIC_CAST(int16_t, tmp));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'name': 'simde_wasm_i16x8_q15mulr_sat',
    'x86_ssse3_mapping': ['    #elif defined(SIMDE_X86_SSSE3_NATIVE)\n', '      __m128i y = _mm_mulhrs_epi16(a_.sse_m128i, b_.sse_m128i);\n', '      __m128i tmp = _mm_cmpeq_epi16(y, _mm_set1_epi16(INT16_MAX));\n', '      r_.sse_m128i = _mm_xor_si128(y, tmp);\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_q15mulr_sat(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n', '    /* https://github.com/WebAssembly/simd/pull/365 */\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vqrdmulhq_s16(a_.neon_i16, b_.neon_i16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      const __m128i prod_lo = _mm_mullo_epi16(a_.sse_m128i, b_.sse_m128i);\n', '      const __m128i prod_hi = _mm_mulhi_epi16(a_.sse_m128i, b_.sse_m128i);\n', '      const __m128i tmp =\n', '        _mm_add_epi16(\n', '          _mm_avg_epu16(\n', '            _mm_srli_epi16(prod_lo, 14),\n', '            _mm_setzero_si128()\n', '          ),\n', '          _mm_add_epi16(prod_hi, prod_hi)\n', '        );\n', '      r_.sse_m128i =\n', '        _mm_xor_si128(\n', '          tmp,\n', '          _mm_cmpeq_epi16(_mm_set1_epi16(INT16_MAX), tmp)\n', '        );\n', '    #else\n']
}, {
    'name': 'simde_wasm_i8x16_min',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = (a_.i8[i] < b_.i8[i]) ? a_.i8[i] : b_.i8[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_min(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vminq_s8(a_.neon_i8, b_.neon_i8);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_min_epi8(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      __m128i m = _mm_cmplt_epi8(a_.sse_m128i, b_.sse_m128i);\n', '      r_.sse_m128i =\n', '        _mm_or_si128(\n', '          _mm_and_si128(m, a_.sse_m128i),\n', '          _mm_andnot_si128(m, b_.sse_m128i)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = vec_min(a_.altivec_i8, b_.altivec_i8);\n', '    #else\n']
}, {
    'name': 'simde_wasm_i16x8_min',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = (a_.i16[i] < b_.i16[i]) ? a_.i16[i] : b_.i16[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_min(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vminq_s16(a_.neon_i16, b_.neon_i16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_min_epi16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = vec_min(a_.altivec_i16, b_.altivec_i16);\n', '    #else\n']
}, {
    'name': 'simde_wasm_i32x4_min',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = (a_.i32[i] < b_.i32[i]) ? a_.i32[i] : b_.i32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_min(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vminq_s32(a_.neon_i32, b_.neon_i32);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_min_epi32(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      __m128i m = _mm_cmplt_epi32(a_.sse_m128i, b_.sse_m128i);\n', '      r_.sse_m128i =\n', '        _mm_or_si128(\n', '          _mm_and_si128(m, a_.sse_m128i),\n', '          _mm_andnot_si128(m, b_.sse_m128i)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 = vec_min(a_.altivec_i32, b_.altivec_i32);\n', '    #else\n']
}, {
    'name': 'simde_wasm_u8x16_min',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u8) / sizeof(r_.u8[0])) ; i++) {\n', '        r_.u8[i] = (a_.u8[i] < b_.u8[i]) ? a_.u8[i] : b_.u8[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_min(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vminq_u8(a_.neon_u8, b_.neon_u8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_min_epu8(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u8 = vec_min(a_.altivec_u8, b_.altivec_u8);\n', '    #else\n']
}, {
    'name': 'simde_wasm_u16x8_min',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = (a_.u16[i] < b_.u16[i]) ? a_.u16[i] : b_.u16[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_min(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vminq_u16(a_.neon_u16, b_.neon_u16);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_min_epu16(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      /* https://github.com/simd-everywhere/simde/issues/855#issuecomment-881656284 */\n', '      r_.sse_m128i = _mm_sub_epi16(a, _mm_subs_epu16(a_.sse_m128i, b_.sse_m128i));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u16 = vec_min(a_.altivec_u16, b_.altivec_u16);\n', '    #else\n']
}, {
    'name': 'simde_wasm_u32x4_min',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.u32[i] = (a_.u32[i] < b_.u32[i]) ? a_.u32[i] : b_.u32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_min(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vminq_u32(a_.neon_u32, b_.neon_u32);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_min_epu32(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      const __m128i i32_min = _mm_set1_epi32(INT32_MIN);\n', '      const __m128i difference = _mm_sub_epi32(a_.sse_m128i, b_.sse_m128i);\n', '      __m128i m =\n', '        _mm_cmpeq_epi32(\n', '          /* _mm_subs_epu32(a_.sse_m128i, b_.sse_m128i) */\n', '          _mm_and_si128(\n', '            difference,\n', '            _mm_xor_si128(\n', '              _mm_cmpgt_epi32(\n', '                _mm_xor_si128(difference, i32_min),\n', '                _mm_xor_si128(a_.sse_m128i, i32_min)\n', '              ),\n', '              _mm_set1_epi32(~INT32_C(0))\n', '            )\n', '          ),\n', '          _mm_setzero_si128()\n', '        );\n', '      r_.sse_m128i =\n', '        _mm_or_si128(\n', '          _mm_and_si128(m, a_.sse_m128i),\n', '          _mm_andnot_si128(m, b_.sse_m128i)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_u32 = vec_min(a_.altivec_u32, b_.altivec_u32);\n', '    #else\n']
}, {
    'powerpc_altivec_p8_mapping': ['      #if defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '        condition = vec_orc(a_lt_b, vec_cmpeq(a_.altivec_f32, a_.altivec_f32));\n', '      #else\n', '        SIMDE_POWER_ALTIVEC_VECTOR(SIMDE_POWER_ALTIVEC_BOOL int) a_not_nan =\n', '          vec_cmpeq(a_.altivec_f32, a_.altivec_f32);\n', '        condition = vec_or(a_lt_b, vec_nor(a_not_nan, a_not_nan));\n', '      #endif\n', '\n', '      r_.altivec_f32 =\n', '        vec_sel(\n', '          b_.altivec_f32,\n', '          a_.altivec_f32,\n', '          condition\n', '        );\n', '    #else\n'],
    'name': 'simde_wasm_f32x4_min',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = (simde_math_isnan(a_.f32[i]) || (a_.f32[i] < b_.f32[i])) ? a_.f32[i] : b_.f32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_min(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_f32 = vminq_f32(a_.neon_f32, b_.neon_f32);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128 = _mm_blendv_ps(\n', '          _mm_set1_ps(SIMDE_MATH_NANF),\n', '          _mm_min_ps(a_.sse_m128, b_.sse_m128),\n', '          _mm_cmpord_ps(a_.sse_m128, b_.sse_m128));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      __m128 m = _mm_cmpord_ps(a_.sse_m128, b_.sse_m128);\n', '      r_.sse_m128 =\n', '        _mm_or_ps(\n', '          _mm_and_ps(m, _mm_min_ps(a_.sse_m128, b_.sse_m128)),\n', '          _mm_andnot_ps(m, _mm_set1_ps(SIMDE_MATH_NANF))\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(SIMDE_POWER_ALTIVEC_BOOL int) condition;\n', '      SIMDE_POWER_ALTIVEC_VECTOR(SIMDE_POWER_ALTIVEC_BOOL int) a_lt_b =\n', '        vec_cmpgt(b_.altivec_f32, a_.altivec_f32);\n', '\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f64 = vminq_f64(a_.neon_f64, b_.neon_f64);\n', '    #else\n'],
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_f64 =\n', '        vec_sel(\n', '          b_.altivec_f64,\n', '          a_.altivec_f64,\n', '          vec_orc(\n', '            vec_cmpgt(b_.altivec_f64, a_.altivec_f64),\n', '            vec_cmpeq(a_.altivec_f64, a_.altivec_f64)\n', '          )\n', '        );\n'],
    'name': 'simde_wasm_f64x2_min',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = (simde_math_isnan(a_.f64[i]) || (a_.f64[i] < b_.f64[i])) ? a_.f64[i] : b_.f64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_min(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128d = _mm_blendv_pd(\n', '          _mm_set1_pd(SIMDE_MATH_NAN),\n', '          _mm_min_pd(a_.sse_m128d, b_.sse_m128d),\n', '          _mm_cmpord_pd(a_.sse_m128d, b_.sse_m128d));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      __m128d m = _mm_cmpord_pd(a_.sse_m128d, b_.sse_m128d);\n', '      r_.sse_m128d =\n', '        _mm_or_pd(\n', '          _mm_and_pd(m, _mm_min_pd(a_.sse_m128d, b_.sse_m128d)),\n', '          _mm_andnot_pd(m, _mm_set1_pd(SIMDE_MATH_NAN))\n', '        );\n']
}, {
    'name': 'simde_wasm_i8x16_max',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = (a_.i8[i] > b_.i8[i]) ? a_.i8[i] : b_.i8[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      __typeof__(r_.i8) m = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i8), a_.i8 > b_.i8);\n', '      r_.i8 = (m & a_.i8) | (~m & b_.i8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_max(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vmaxq_s8(a_.neon_i8, b_.neon_i8);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_max_epi8(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      __m128i m = _mm_cmpgt_epi8(a_.sse_m128i, b_.sse_m128i);\n', '      r_.sse_m128i = _mm_or_si128(_mm_and_si128(m, a_.sse_m128i), _mm_andnot_si128(m, b_.sse_m128i));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_i8 = vec_max(a_.altivec_i8, b_.altivec_i8);\n']
}, {
    'name': 'simde_wasm_i16x8_max',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = (a_.i16[i] > b_.i16[i]) ? a_.i16[i] : b_.i16[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      __typeof__(r_.i16) m = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i16), a_.i16 > b_.i16);\n', '      r_.i16 = (m & a_.i16) | (~m & b_.i16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_max(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vmaxq_s16(a_.neon_i16, b_.neon_i16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_max_epi16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_i16 = vec_max(a_.altivec_i16, b_.altivec_i16);\n']
}, {
    'name': 'simde_wasm_i32x4_max',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = (a_.i32[i] > b_.i32[i]) ? a_.i32[i] : b_.i32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      __typeof__(r_.i32) m = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.i32 > b_.i32);\n', '      r_.i32 = (m & a_.i32) | (~m & b_.i32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_max(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vmaxq_s32(a_.neon_i32, b_.neon_i32);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_max_epi32(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      __m128i m = _mm_cmpgt_epi32(a_.sse_m128i, b_.sse_m128i);\n', '      r_.sse_m128i = _mm_or_si128(_mm_and_si128(m, a_.sse_m128i), _mm_andnot_si128(m, b_.sse_m128i));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_i32 = vec_max(a_.altivec_i32, b_.altivec_i32);\n']
}, {
    'name': 'simde_wasm_u8x16_max',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u8) / sizeof(r_.u8[0])) ; i++) {\n', '        r_.u8[i] = (a_.u8[i] > b_.u8[i]) ? a_.u8[i] : b_.u8[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      __typeof__(r_.u8) m = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u8), a_.u8 > b_.u8);\n', '      r_.u8 = (m & a_.u8) | (~m & b_.u8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_max(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vmaxq_u8(a_.neon_u8, b_.neon_u8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_max_epu8(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_u8 = vec_max(a_.altivec_u8, b_.altivec_u8);\n']
}, {
    'name': 'simde_wasm_u16x8_max',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = (a_.u16[i] > b_.u16[i]) ? a_.u16[i] : b_.u16[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      __typeof__(r_.u16) m = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u16), a_.u16 > b_.u16);\n', '      r_.u16 = (m & a_.u16) | (~m & b_.u16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_max(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vmaxq_u16(a_.neon_u16, b_.neon_u16);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_max_epu16(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      /* https://github.com/simd-everywhere/simde/issues/855#issuecomment-881656284 */\n', '      r_.sse_m128i = _mm_add_epi16(b, _mm_subs_epu16(a_.sse_m128i, b_.sse_m128i));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_u16 = vec_max(a_.altivec_u16, b_.altivec_u16);\n']
}, {
    'name': 'simde_wasm_u32x4_max',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.u32[i] = (a_.u32[i] > b_.u32[i]) ? a_.u32[i] : b_.u32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      __typeof__(r_.u32) m = HEDLEY_REINTERPRET_CAST(__typeof__(r_.u32), a_.u32 > b_.u32);\n', '      r_.u32 = (m & a_.u32) | (~m & b_.u32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_max(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vmaxq_u32(a_.neon_u32, b_.neon_u32);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_max_epu32(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      /* https://github.com/simd-everywhere/simde/issues/855#issuecomment-886057227 */\n', '      __m128i m =\n', '        _mm_xor_si128(\n', '          _mm_cmpgt_epi32(a_.sse_m128i, b_.sse_m128i),\n', '          _mm_srai_epi32(_mm_xor_si128(a_.sse_m128i, b_.sse_m128i), 31)\n', '        );\n', '      r_.sse_m128i = _mm_or_si128(_mm_and_si128(m, a_.sse_m128i), _mm_andnot_si128(m, b_.sse_m128i));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_u32 = vec_max(a_.altivec_u32, b_.altivec_u32);\n']
}, {
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_f32 =\n', '        vec_sel(\n', '          b_.altivec_f32,\n', '          a_.altivec_f32,\n', '          vec_orc(\n', '            vec_cmpgt(a_.altivec_f32, b_.altivec_f32),\n', '            vec_cmpeq(a_.altivec_f32, a_.altivec_f32)\n', '          )\n', '        );\n'],
    'name': 'simde_wasm_f32x4_max',
    'x86_sse_mapping': ['    #elif defined(SIMDE_X86_SSE_NATIVE)\n', '      __m128 m = _mm_or_ps(_mm_cmpneq_ps(a_.sse_m128, a_.sse_m128), _mm_cmpgt_ps(a_.sse_m128, b_.sse_m128));\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = (simde_math_isnan(a_.f32[i]) || (a_.f32[i] > b_.f32[i])) ? a_.f32[i] : b_.f32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      int32_t SIMDE_VECTOR(16) m = HEDLEY_REINTERPRET_CAST(__typeof__(m), (a_.f32 != a_.f32) | (a_.f32 > b_.f32));\n', '      r_.f32 =\n', '        HEDLEY_REINTERPRET_CAST(\n', '          __typeof__(r_.f32),\n', '          (\n', '            ( m & HEDLEY_REINTERPRET_CAST(__typeof__(m), a_.f32)) |\n', '            (~m & HEDLEY_REINTERPRET_CAST(__typeof__(m), b_.f32))\n', '          )\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_max(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_f32 = vmaxq_f32(a_.neon_f32, b_.neon_f32);\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128 = _mm_blendv_ps(\n', '          _mm_set1_ps(SIMDE_MATH_NANF),\n', '          _mm_max_ps(a_.sse_m128, b_.sse_m128),\n', '          _mm_cmpord_ps(a_.sse_m128, b_.sse_m128));\n', '      #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '        r_.ssse_m128 = _mm_blendv_ps(b_.sse_m128, a_.sse_m128, m);\n', '      #else\n', '        r_.sse_m128 =\n', '          _mm_or_ps(\n', '            _mm_and_ps(m, a_.sse_m128),\n', '            _mm_andnot_ps(m, b_.sse_m128)\n', '          );\n', '      #endif\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(SIMDE_POWER_ALTIVEC_BOOL int) cmpres = vec_cmpeq(a_.altivec_f32, a_.altivec_f32);\n', '      r_.altivec_f32 =\n', '        vec_sel(\n', '          b_.altivec_f32,\n', '          a_.altivec_f32,\n', '          vec_or(\n', '            vec_cmpgt(a_.altivec_f32, b_.altivec_f32),\n', '            vec_nor(cmpres, cmpres)\n', '          )\n', '        );\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f64 = vmaxq_f64(a_.neon_f64, b_.neon_f64);\n'],
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_f64 =\n', '        vec_sel(\n', '          b_.altivec_f64,\n', '          a_.altivec_f64,\n', '          vec_orc(\n', '            vec_cmpgt(a_.altivec_f64, b_.altivec_f64),\n', '            vec_cmpeq(a_.altivec_f64, a_.altivec_f64)\n', '          )\n', '        );\n'],
    'name': 'simde_wasm_f64x2_max',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(SIMDE_POWER_ALTIVEC_BOOL long long) cmpres = vec_cmpeq(a_.altivec_f64, a_.altivec_f64);\n', '      r_.altivec_f64 =\n', '        vec_sel(\n', '          b_.altivec_f64,\n', '          a_.altivec_f64,\n', '          vec_or(\n', '            vec_cmpgt(a_.altivec_f64, b_.altivec_f64),\n', '            vec_nor(cmpres, cmpres)\n', '          )\n', '        );\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = (simde_math_isnan(a_.f64[i]) || (a_.f64[i] > b_.f64[i])) ? a_.f64[i] : b_.f64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      int64_t SIMDE_VECTOR(16) m = HEDLEY_REINTERPRET_CAST(__typeof__(m), (a_.f64 != a_.f64) | (a_.f64 > b_.f64));\n', '      r_.f64 =\n', '        HEDLEY_REINTERPRET_CAST(\n', '          __typeof__(r_.f64),\n', '          (\n', '            ( m & HEDLEY_REINTERPRET_CAST(__typeof__(m), a_.f64)) |\n', '            (~m & HEDLEY_REINTERPRET_CAST(__typeof__(m), b_.f64))\n', '          )\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_max(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128d = _mm_blendv_pd(\n', '          _mm_set1_pd(SIMDE_MATH_NAN),\n', '          _mm_max_pd(a_.sse_m128d, b_.sse_m128d),\n', '          _mm_cmpord_pd(a_.sse_m128d, b_.sse_m128d));\n', '      #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '        r_.ssse_m128d = _mm_blendv_pd(b_.sse_m128d, a_.sse_m128d, m);\n', '      #else\n', '        r_.sse_m128d =\n', '          _mm_or_pd(\n', '            _mm_and_pd(m, a_.sse_m128d),\n', '            _mm_andnot_pd(m, b_.sse_m128d)\n', '          );\n', '      #endif\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      __m128d m = _mm_or_pd(_mm_cmpneq_pd(a_.sse_m128d, a_.sse_m128d), _mm_cmpgt_pd(a_.sse_m128d, b_.sse_m128d));\n']
}, {
    'name': 'simde_wasm_i8x16_add_sat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = simde_math_adds_i8(a_.i8[i], b_.i8[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      __typeof__(a_.u8) r1, r2, m;\n', '      r1 = a_.u8 + b_.u8;\n', '      r2 = (a_.u8 >> 7) + INT8_MAX;\n', '      m = HEDLEY_REINTERPRET_CAST(__typeof__(m), HEDLEY_REINTERPRET_CAST(__typeof__(r_.i8), (r2 ^ b_.u8) | ~(b_.u8 ^ r1)) < 0);\n', '      r_.i8 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i8), (r1 & m) | (r2 & ~m));\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_add_sat(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vqaddq_s8(a_.neon_i8, b_.neon_i8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_adds_epi8(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = vec_adds(a_.altivec_i8, b_.altivec_i8);\n']
}, {
    'name': 'simde_wasm_i16x8_add_sat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = simde_math_adds_i16(a_.i16[i], b_.i16[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      __typeof__(a_.u16) r1, r2, m;\n', '      r1 = a_.u16 + b_.u16;\n', '      r2 = (a_.u16 >> 15) + INT16_MAX;\n', '      m = HEDLEY_REINTERPRET_CAST(__typeof__(m), HEDLEY_REINTERPRET_CAST(__typeof__(r_.i16), (r2 ^ b_.u16) | ~(b_.u16 ^ r1)) < 0);\n', '      r_.i16 = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i16), (r1 & m) | (r2 & ~m));\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_add_sat(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vqaddq_s16(a_.neon_i16, b_.neon_i16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_adds_epi16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = vec_adds(a_.altivec_i16, b_.altivec_i16);\n']
}, {
    'name': 'simde_wasm_u8x16_add_sat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u8) / sizeof(r_.u8[0])) ; i++) {\n', '        r_.u8[i] = simde_math_adds_u8(a_.u8[i], b_.u8[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      r_.u8 = a_.u8 + b_.u8;\n', '      r_.u8 |= HEDLEY_REINTERPRET_CAST(__typeof__(r_.u8), r_.u8 < a_.u8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_add_sat(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vqaddq_u8(a_.neon_u8, b_.neon_u8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_adds_epu8(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u8 = vec_adds(a_.altivec_u8, b_.altivec_u8);\n']
}, {
    'name': 'simde_wasm_u16x8_add_sat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = simde_math_adds_u16(a_.u16[i], b_.u16[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      r_.u16 = a_.u16 + b_.u16;\n', '      r_.u16 |= HEDLEY_REINTERPRET_CAST(__typeof__(r_.u16), r_.u16 < a_.u16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_add_sat(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vqaddq_u16(a_.neon_u16, b_.neon_u16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_adds_epu16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u16 = vec_adds(a_.altivec_u16, b_.altivec_u16);\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u8) / sizeof(r_.u8[0])) ; i++) {\n', '        r_.u8[i] = (a_.u8[i] + b_.u8[i] + 1) >> 1;\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_avgr(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_u8x16_avgr',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_avg_epu8(a_.sse_m128i, b_.sse_m128i);\n', '    #else\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = (a_.u16[i] + b_.u16[i] + 1) >> 1;\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_avgr(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_u16x8_avgr',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_avg_epu16(a_.sse_m128i, b_.sse_m128i);\n', '    #else\n']
}, {
    'name': 'simde_wasm_i8x16_sub_sat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = simde_math_subs_i8(a_.i8[i], b_.i8[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      const __typeof__(r_.i8) diff_sat = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i8), (b_.i8 > a_.i8) ^ INT8_MAX);\n', '      const __typeof__(r_.i8) diff = a_.i8 - b_.i8;\n', '      const __typeof__(r_.i8) saturate = diff_sat ^ diff;\n', '      const __typeof__(r_.i8) m = saturate >> 7;\n', '      r_.i8 = (diff_sat & m) | (diff & ~m);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_sub_sat(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vqsubq_s8(a_.neon_i8, b_.neon_i8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_subs_epi8(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = vec_subs(a_.altivec_i8, b_.altivec_i8);\n']
}, {
    'name': 'simde_wasm_i16x8_sub_sat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = simde_math_subs_i16(a_.i16[i], b_.i16[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      const __typeof__(r_.i16) diff_sat = HEDLEY_REINTERPRET_CAST(__typeof__(r_.i16), (b_.i16 > a_.i16) ^ INT16_MAX);\n', '      const __typeof__(r_.i16) diff = a_.i16 - b_.i16;\n', '      const __typeof__(r_.i16) saturate = diff_sat ^ diff;\n', '      const __typeof__(r_.i16) m = saturate >> 15;\n', '      r_.i16 = (diff_sat & m) | (diff & ~m);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_sub_sat(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vqsubq_s16(a_.neon_i16, b_.neon_i16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_subs_epi16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = vec_subs(a_.altivec_i16, b_.altivec_i16);\n']
}, {
    'name': 'simde_wasm_u8x16_sub_sat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u8) / sizeof(r_.u8[0])) ; i++) {\n', '        r_.u8[i] = simde_math_subs_u8(a_.u8[i], b_.u8[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      r_.u8  = a_.u8 - b_.u8;\n', '      r_.u8 &= HEDLEY_REINTERPRET_CAST(__typeof__(r_.u8), r_.u8 <= a_.u8);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_sub_sat(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 = vqsubq_u8(a_.neon_u8, b_.neon_u8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_subs_epu8(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u8 = vec_subs(a_.altivec_u8, b_.altivec_u8);\n']
}, {
    'name': 'simde_wasm_u16x8_sub_sat',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = simde_math_subs_u16(a_.u16[i], b_.u16[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      r_.u16  = a_.u16 - b_.u16;\n', '      r_.u16 &= HEDLEY_REINTERPRET_CAST(__typeof__(r_.u16), r_.u16 <= a_.u16);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_sub_sat(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vqsubq_u16(a_.neon_u16, b_.neon_u16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_subs_epu16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u16 = vec_subs(a_.altivec_u16, b_.altivec_u16);\n']
}, {
    'name': 'simde_wasm_f32x4_pmin',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = (b_.f32[i] < a_.f32[i]) ? b_.f32[i] : a_.f32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_pmin(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_f32 =\n', '        vbslq_f32(\n', '          vcltq_f32(b_.neon_f32, a_.neon_f32),\n', '          b_.neon_f32,\n', '          a_.neon_f32\n', '        );\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_min_ps(b_.sse_m128, a_.sse_m128);\n', '    #elif defined(SIMDE_FAST_NANS) && defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_f32 = vminq_f32(a_.neon_f32, b_.neon_f32);\n', '    #elif defined(SIMDE_FAST_NANS) && defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_f32 = vec_min(a_.altivec_f32, b_.altivec_f32);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_f32 =\n', '        vec_sel(\n', '          a_.altivec_f32,\n', '          b_.altivec_f32,\n', '          vec_cmpgt(a_.altivec_f32, b_.altivec_f32)\n', '        );\n', '    #else\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f64 =\n', '        vbslq_f64(\n', '          vcltq_f64(b_.neon_f64, a_.neon_f64),\n', '          b_.neon_f64,\n', '          a_.neon_f64\n', '        );\n'],
    'name': 'simde_wasm_f64x2_pmin',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_f32 =\n', '        vec_sel(\n', '          a_.altivec_f32,\n', '          b_.altivec_f32,\n', '          vec_cmpgt(a_.altivec_f32, b_.altivec_f32)\n', '        );\n', '    #else\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = (b_.f64[i] < a_.f64[i]) ? b_.f64[i] : a_.f64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_pmin(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_min_pd(b_.sse_m128d, a_.sse_m128d);\n', '    #elif defined(SIMDE_FAST_NANS) && defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f32 = vminq_f64(a_.neon_f64, b_.neon_f64);\n', '    #elif defined(SIMDE_FAST_NANS) && defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_f64 = vec_min(a_.altivec_f64, b_.altivec_f64);\n']
}, {
    'name': 'simde_wasm_f32x4_pmax',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = (a_.f32[i] < b_.f32[i]) ? b_.f32[i] : a_.f32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      int32_t SIMDE_VECTOR(16) m = HEDLEY_REINTERPRET_CAST(__typeof__(m), a_.f32 < b_.f32);\n', '      r_.f32 =\n', '        HEDLEY_REINTERPRET_CAST(\n', '          __typeof__(r_.f32),\n', '          (\n', '            ( m & HEDLEY_REINTERPRET_CAST(__typeof__(m), b_.f32)) |\n', '            (~m & HEDLEY_REINTERPRET_CAST(__typeof__(m), a_.f32))\n', '          )\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_pmax(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_f32 = vbslq_f32(vcltq_f32(a_.neon_f32, b_.neon_f32), b_.neon_f32, a_.neon_f32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_max_ps(b_.sse_m128, a_.sse_m128);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_14_NATIVE)\n', '      r_.altivec_f32 = vec_sel(a_.altivec_f32, b_.altivec_f32, vec_cmplt(a_.altivec_f32, b_.altivec_f32));\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f64 = vbslq_f64(vcltq_f64(a_.neon_f64, b_.neon_f64), b_.neon_f64, a_.neon_f64);\n'],
    'name': 'simde_wasm_f64x2_pmax',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_f64 = vec_sel(a_.altivec_f64, b_.altivec_f64, vec_cmplt(a_.altivec_f64, b_.altivec_f64));\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = (a_.f64[i] < b_.f64[i]) ? b_.f64[i] : a_.f64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS)\n', '      int64_t SIMDE_VECTOR(16) m = HEDLEY_REINTERPRET_CAST(__typeof__(m), a_.f64 < b_.f64);\n', '      r_.f64 =\n', '        HEDLEY_REINTERPRET_CAST(\n', '          __typeof__(r_.f64),\n', '          (\n', '            ( m & HEDLEY_REINTERPRET_CAST(__typeof__(m), b_.f64)) |\n', '            (~m & HEDLEY_REINTERPRET_CAST(__typeof__(m), a_.f64))\n', '          )\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_pmax(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_max_pd(b_.sse_m128d, a_.sse_m128d);\n']
}, {
    'name': 'simde_wasm_f32x4_div',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = a_.f32[i] / b_.f32[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.f32 = a_.f32 / b_.f32;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_div(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_div_ps(a_.sse_m128, b_.sse_m128);\n']
}, {
    'name': 'simde_wasm_f64x2_div',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = a_.f64[i] / b_.f64[i];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT)\n', '      r_.f64 = a_.f64 / b_.f64;\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_div(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_div_pd(a_.sse_m128d, b_.sse_m128d);\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define \\\n', '    simde_wasm_i8x16_shuffle( \\\n', '        a, b, \\\n', '        c0, c1,  c2,  c3,  c4,  c5,  c6,  c7, \\\n', '        c8, c9, c10, c11, c12, c13, c14, c15) \\\n', '    wasm_i8x16_shuffle( \\\n', '        a, b, \\\n', '        c0, c1,  c2,  c3,  c4,  c5,  c6,  c7, \\\n', '        c8, c9, c10, c11, c12, c13, c14, c15)\n', '#elif defined(SIMDE_SHUFFLE_VECTOR_)\n', '  #define \\\n', '    simde_wasm_i8x16_shuffle( \\\n', '        a, b, \\\n', '        c0, c1,  c2,  c3,  c4,  c5,  c6,  c7, \\\n', '        c8, c9, c10, c11, c12, c13, c14, c15) \\\n', '    (__extension__ ({ \\\n', '      HEDLEY_REINTERPRET_CAST(simde_v128_t, SIMDE_SHUFFLE_VECTOR_(8, 16, \\\n', '          HEDLEY_REINTERPRET_CAST(int8_t SIMDE_VECTOR(16), a), \\\n', '          HEDLEY_REINTERPRET_CAST(int8_t SIMDE_VECTOR(16), b), \\\n', '          c0, c1,  c2,  c3,  c4,  c5,  c6,  c7, \\\n', '          c8, c9, c10, c11, c12, c13, c14, c15)); \\\n', '    }))\n', '#endif\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i8x16_shuffle',
    'tbd': ['    simde_v128_t a, simde_v128_t b,\n', '    const int c0, const int c1, const int  c2, const int  c3, const int  c4, const int  c5, const int  c6, const int  c7,\n', '    const int c8, const int c9, const int c10, const int c11, const int c12, const int c13, const int c14, const int c15) {\n', '  simde_v128_private\n', '    a_ = simde_v128_to_private(a),\n', '    b_ = simde_v128_to_private(b),\n', '    r_;\n', '\n', '  r_.i8[ 0] = ( c0 < 16) ? a_.i8[ c0] : b_.i8[ c0 & 15];\n', '  r_.i8[ 1] = ( c1 < 16) ? a_.i8[ c1] : b_.i8[ c1 & 15];\n', '  r_.i8[ 2] = ( c2 < 16) ? a_.i8[ c2] : b_.i8[ c2 & 15];\n', '  r_.i8[ 3] = ( c3 < 16) ? a_.i8[ c3] : b_.i8[ c3 & 15];\n', '  r_.i8[ 4] = ( c4 < 16) ? a_.i8[ c4] : b_.i8[ c4 & 15];\n', '  r_.i8[ 5] = ( c5 < 16) ? a_.i8[ c5] : b_.i8[ c5 & 15];\n', '  r_.i8[ 6] = ( c6 < 16) ? a_.i8[ c6] : b_.i8[ c6 & 15];\n', '  r_.i8[ 7] = ( c7 < 16) ? a_.i8[ c7] : b_.i8[ c7 & 15];\n', '  r_.i8[ 8] = ( c8 < 16) ? a_.i8[ c8] : b_.i8[ c8 & 15];\n', '  r_.i8[ 9] = ( c9 < 16) ? a_.i8[ c9] : b_.i8[ c9 & 15];\n', '  r_.i8[10] = (c10 < 16) ? a_.i8[c10] : b_.i8[c10 & 15];\n', '  r_.i8[11] = (c11 < 16) ? a_.i8[c11] : b_.i8[c11 & 15];\n', '  r_.i8[12] = (c12 < 16) ? a_.i8[c12] : b_.i8[c12 & 15];\n', '  r_.i8[13] = (c13 < 16) ? a_.i8[c13] : b_.i8[c13 & 15];\n', '  r_.i8[14] = (c14 < 16) ? a_.i8[c14] : b_.i8[c14 & 15];\n', '  r_.i8[15] = (c15 < 16) ? a_.i8[c15] : b_.i8[c15 & 15];\n', '\n', '  return simde_v128_from_private(r_);\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define \\\n', '    simde_wasm_i16x8_shuffle( \\\n', '        a, b, \\\n', '        c0, c1,  c2,  c3,  c4,  c5,  c6,  c7) \\\n', '    wasm_i16x8_shuffle( \\\n', '        a, b, \\\n', '        c0, c1,  c2,  c3,  c4,  c5,  c6,  c7)\n', '#elif defined(SIMDE_SHUFFLE_VECTOR_)\n', '  #define \\\n', '    simde_wasm_i16x8_shuffle( \\\n', '        a, b, \\\n', '        c0, c1,  c2,  c3,  c4,  c5,  c6,  c7) \\\n', '    (__extension__ ({ \\\n', '      HEDLEY_REINTERPRET_CAST(simde_v128_t, SIMDE_SHUFFLE_VECTOR_(16, 16, \\\n', '          HEDLEY_REINTERPRET_CAST(int16_t SIMDE_VECTOR(16), a), \\\n', '          HEDLEY_REINTERPRET_CAST(int16_t SIMDE_VECTOR(16), b), \\\n', '          c0, c1,  c2,  c3,  c4,  c5,  c6,  c7)); \\\n', '    }))\n', '#endif\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i16x8_shuffle',
    'tbd': ['    simde_v128_t a, simde_v128_t b,\n', '    const int c0, const int c1, const int  c2, const int  c3, const int  c4, const int  c5, const int  c6, const int  c7) {\n', '  simde_v128_private\n', '    a_ = simde_v128_to_private(a),\n', '    b_ = simde_v128_to_private(b),\n', '    r_;\n', '\n', '  r_.i16[ 0] = (c0 < 8) ? a_.i16[ c0] : b_.i16[ c0 & 7];\n', '  r_.i16[ 1] = (c1 < 8) ? a_.i16[ c1] : b_.i16[ c1 & 7];\n', '  r_.i16[ 2] = (c2 < 8) ? a_.i16[ c2] : b_.i16[ c2 & 7];\n', '  r_.i16[ 3] = (c3 < 8) ? a_.i16[ c3] : b_.i16[ c3 & 7];\n', '  r_.i16[ 4] = (c4 < 8) ? a_.i16[ c4] : b_.i16[ c4 & 7];\n', '  r_.i16[ 5] = (c5 < 8) ? a_.i16[ c5] : b_.i16[ c5 & 7];\n', '  r_.i16[ 6] = (c6 < 8) ? a_.i16[ c6] : b_.i16[ c6 & 7];\n', '  r_.i16[ 7] = (c7 < 8) ? a_.i16[ c7] : b_.i16[ c7 & 7];\n', '\n', '  return simde_v128_from_private(r_);\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define \\\n', '    simde_wasm_i32x4_shuffle( \\\n', '        a, b, \\\n', '        c0, c1,  c2,  c3) \\\n', '    wasm_i32x4_shuffle( \\\n', '        a, b, \\\n', '        c0, c1,  c2,  c3)\n', '#elif defined(SIMDE_SHUFFLE_VECTOR_)\n', '  #define \\\n', '    simde_wasm_i32x4_shuffle( \\\n', '        a, b, \\\n', '        c0, c1,  c2,  c3) \\\n', '    (__extension__ ({ \\\n', '      HEDLEY_REINTERPRET_CAST(simde_v128_t, SIMDE_SHUFFLE_VECTOR_(32, 16, \\\n', '          HEDLEY_REINTERPRET_CAST(int32_t SIMDE_VECTOR(16), a), \\\n', '          HEDLEY_REINTERPRET_CAST(int32_t SIMDE_VECTOR(16), b), \\\n', '          c0, c1,  c2,  c3)); \\\n', '    }))\n', '#endif\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i32x4_shuffle',
    'tbd': ['    simde_v128_t a, simde_v128_t b,\n', '    const int c0, const int c1, const int  c2, const int  c3) {\n', '  simde_v128_private\n', '    a_ = simde_v128_to_private(a),\n', '    b_ = simde_v128_to_private(b),\n', '    r_;\n', '\n', '  r_.i32[ 0] = (c0 < 4) ? a_.i32[ c0] : b_.i32[ c0 & 3];\n', '  r_.i32[ 1] = (c1 < 4) ? a_.i32[ c1] : b_.i32[ c1 & 3];\n', '  r_.i32[ 2] = (c2 < 4) ? a_.i32[ c2] : b_.i32[ c2 & 3];\n', '  r_.i32[ 3] = (c3 < 4) ? a_.i32[ c3] : b_.i32[ c3 & 3];\n', '\n', '  return simde_v128_from_private(r_);\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define \\\n', '    simde_wasm_i64x2_shuffle( \\\n', '        a, b, \\\n', '        c0, c1) \\\n', '    wasm_i64x2_shuffle( \\\n', '        a, b, \\\n', '        c0, c1)\n', '#elif defined(SIMDE_SHUFFLE_VECTOR_)\n', '  #define \\\n', '    simde_wasm_i64x2_shuffle( \\\n', '        a, b, \\\n', '        c0, c1) \\\n', '    (__extension__ ({ \\\n', '      HEDLEY_REINTERPRET_CAST(simde_v128_t, SIMDE_SHUFFLE_VECTOR_(64, 16, \\\n', '          HEDLEY_REINTERPRET_CAST(int64_t SIMDE_VECTOR(16), a), \\\n', '          HEDLEY_REINTERPRET_CAST(int64_t SIMDE_VECTOR(16), b), \\\n', '          c0, c1)); \\\n', '    }))\n', '#endif\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i64x2_shuffle',
    'tbd': ['    simde_v128_t a, simde_v128_t b,\n', '    const int c0, const int c1) {\n', '  simde_v128_private\n', '    a_ = simde_v128_to_private(a),\n', '    b_ = simde_v128_to_private(b),\n', '    r_;\n', '\n', '  r_.i64[ 0] = (c0 < 2) ? a_.i64[ c0] : b_.i64[ c0 & 1];\n', '  r_.i64[ 1] = (c1 < 2) ? a_.i64[ c1] : b_.i64[ c1 & 1];\n', '\n', '  return simde_v128_from_private(r_);\n', '}\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        r_.i8[i] = (b_.u8[i] > 15) ? INT8_C(0) : a_.i8[b_.u8[i]];\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'name': 'simde_wasm_i8x16_swizzle',
    'x86_ssse3_mapping': ['    #elif defined(SIMDE_X86_SSSE3_NATIVE)\n', '      /* https://github.com/WebAssembly/simd/issues/68#issuecomment-470825324 */\n', '      r_.sse_m128i =\n', '        _mm_shuffle_epi8(\n', '          a_.sse_m128i,\n', '          _mm_adds_epu8(\n', '            _mm_set1_epi8(0x70),\n', '            b_.sse_m128i));\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_swizzle(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      int8x8x2_t tmp = { { vget_low_s8(a_.neon_i8), vget_high_s8(a_.neon_i8) } };\n', '      r_.neon_i8 = vcombine_s8(\n', '        vtbl2_s8(tmp, vget_low_s8(b_.neon_i8)),\n', '        vtbl2_s8(tmp, vget_high_s8(b_.neon_i8))\n', '      );\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = vec_perm(\n', '        a_.altivec_i8,\n', '        a_.altivec_i8,\n', '        b_.altivec_u8\n', '      );\n', '      r_.altivec_i8 = vec_and(r_.altivec_i8, vec_cmple(b_.altivec_u8, vec_splat_u8(15)));\n', '    #else\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_i8 = vqmovn_high_s16(vqmovn_s16(a_.neon_i16), b_.neon_i16);\n'],
    'name': 'simde_wasm_i8x16_narrow_i16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        int16_t v = (i < (sizeof(a_.i16) / sizeof(a_.i16[0]))) ? a_.i16[i] : b_.i16[i & 7];\n', '        r_.i8[i] = (v < INT8_MIN) ? INT8_MIN : ((v > INT8_MAX) ? INT8_MAX : HEDLEY_STATIC_CAST(int8_t, v));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      int16_t SIMDE_VECTOR(32) v = SIMDE_SHUFFLE_VECTOR_(16, 32, a_.i16, b_.i16, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);\n', '      const int16_t SIMDE_VECTOR(32) min = { INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN, INT8_MIN };\n', '      const int16_t SIMDE_VECTOR(32) max = { INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX, INT8_MAX };\n', '\n', '      int16_t m SIMDE_VECTOR(32);\n', '      m = HEDLEY_REINTERPRET_CAST(__typeof__(m), v < min);\n', '      v = (v & ~m) | (min & m);\n', '\n', '      m = v > max;\n', '      v = (v & ~m) | (max & m);\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i8, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_narrow_i16x8(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vcombine_s8(vqmovn_s16(a_.neon_i16), vqmovn_s16(b_.neon_i16));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_packs_epi16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = vec_packs(a_.altivec_i16, b_.altivec_i16);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_i16 = vqmovn_high_s32(vqmovn_s32(a_.neon_i32), b_.neon_i32);\n'],
    'name': 'simde_wasm_i16x8_narrow_i32x4',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        int32_t v = (i < (sizeof(a_.i32) / sizeof(a_.i32[0]))) ? a_.i32[i] : b_.i32[i & 3];\n', '        r_.i16[i] = (v < INT16_MIN) ? INT16_MIN : ((v > INT16_MAX) ? INT16_MAX : HEDLEY_STATIC_CAST(int16_t, v));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      int32_t SIMDE_VECTOR(32) v = SIMDE_SHUFFLE_VECTOR_(32, 32, a_.i32, b_.i32, 0, 1, 2, 3, 4, 5, 6, 7);\n', '      const int32_t SIMDE_VECTOR(32) min = { INT16_MIN, INT16_MIN, INT16_MIN, INT16_MIN, INT16_MIN, INT16_MIN, INT16_MIN, INT16_MIN };\n', '      const int32_t SIMDE_VECTOR(32) max = { INT16_MAX, INT16_MAX, INT16_MAX, INT16_MAX, INT16_MAX, INT16_MAX, INT16_MAX, INT16_MAX };\n', '\n', '      int32_t m SIMDE_VECTOR(32);\n', '      m = HEDLEY_REINTERPRET_CAST(__typeof__(m), v < min);\n', '      v = (v & ~m) | (min & m);\n', '\n', '      m = HEDLEY_REINTERPRET_CAST(__typeof__(m), v > max);\n', '      v = (v & ~m) | (max & m);\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i16, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_narrow_i32x4(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vcombine_s16(vqmovn_s32(a_.neon_i32), vqmovn_s32(b_.neon_i32));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_packs_epi32(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = vec_packs(a_.altivec_i32, b_.altivec_i32);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      #if defined(SIMDE_BUG_CLANG_46840)\n', '        r_.neon_u8 = vqmovun_high_s16(vreinterpret_s8_u8(vqmovun_s16(a_.neon_i16)), b_.neon_i16);\n', '      #else\n', '        r_.neon_u8 = vqmovun_high_s16(vqmovun_s16(a_.neon_i16), b_.neon_i16);\n', '      #endif\n'],
    'name': 'simde_wasm_u8x16_narrow_i16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i8) / sizeof(r_.i8[0])) ; i++) {\n', '        int16_t v = (i < (sizeof(a_.i16) / sizeof(a_.i16[0]))) ? a_.i16[i] : b_.i16[i & 7];\n', '        r_.u8[i] = (v < 0) ? UINT8_C(0) : ((v > UINT8_MAX) ? UINT8_MAX : HEDLEY_STATIC_CAST(uint8_t, v));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector) && defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      int16_t v SIMDE_VECTOR(32) = SIMDE_SHUFFLE_VECTOR_(16, 32, a_.i16, b_.i16, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);\n', '\n', '      v &= ~(v >> 15);\n', '      v |= HEDLEY_REINTERPRET_CAST(__typeof__(v), v > UINT8_MAX);\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i8, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u8x16_narrow_i16x8(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u8 =\n', '        vcombine_u8(\n', '          vqmovun_s16(a_.neon_i16),\n', '          vqmovun_s16(b_.neon_i16)\n', '        );\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_packus_epi16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u8 = vec_packsu(a_.altivec_i16, b_.altivec_i16);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      #if defined(SIMDE_BUG_CLANG_46840)\n', '        r_.neon_u16 = vqmovun_high_s32(vreinterpret_s16_u16(vqmovun_s32(a_.neon_i32)), b_.neon_i32);\n', '      #else\n', '        r_.neon_u16 = vqmovun_high_s32(vqmovun_s32(a_.neon_i32), b_.neon_i32);\n', '      #endif\n'],
    'name': 'simde_wasm_u16x8_narrow_i32x4',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        int32_t v = (i < (sizeof(a_.i32) / sizeof(a_.i32[0]))) ? a_.i32[i] : b_.i32[i & 3];\n', '        r_.u16[i] = (v < 0) ? UINT16_C(0) : ((v > UINT16_MAX) ? UINT16_MAX : HEDLEY_STATIC_CAST(uint16_t, v));\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector) && defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      int32_t v SIMDE_VECTOR(32) = SIMDE_SHUFFLE_VECTOR_(32, 32, a_.i32, b_.i32, 0, 1, 2, 3, 4, 5, 6, 7);\n', '\n', '      v &= ~(v >> 31);\n', '      v |= HEDLEY_REINTERPRET_CAST(__typeof__(v), v > UINT16_MAX);\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i16, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_narrow_i32x4(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 =\n', '        vcombine_u16(\n', '          vqmovun_s32(a_.neon_i32),\n', '          vqmovun_s32(b_.neon_i32)\n', '        );\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_packus_epi32(a_.sse_m128i, b_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      const __m128i max = _mm_set1_epi32(UINT16_MAX);\n', '      const __m128i tmpa = _mm_andnot_si128(_mm_srai_epi32(a_.sse_m128i, 31), a_.sse_m128i);\n', '      const __m128i tmpb = _mm_andnot_si128(_mm_srai_epi32(b_.sse_m128i, 31), b_.sse_m128i);\n', '      r_.sse_m128i =\n', '        _mm_packs_epi32(\n', '          _mm_srai_epi32(_mm_slli_epi32(_mm_or_si128(tmpa, _mm_cmpgt_epi32(tmpa, max)), 16), 16),\n', '          _mm_srai_epi32(_mm_slli_epi32(_mm_or_si128(tmpb, _mm_cmpgt_epi32(tmpb, max)), 16), 16)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u16 = vec_packsu(a_.altivec_i32, b_.altivec_i32);\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f32 = vcombine_f32(vcvt_f32_f64(a_.neon_f64), vdup_n_f32(0.0f));\n'],
    'powerpc_altivec_p8_mapping': ['      #if defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '        r_.altivec_f32 =\n', '          HEDLEY_REINTERPRET_CAST(\n', '            SIMDE_POWER_ALTIVEC_VECTOR(float),\n', '            vec_pack(\n', '              HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(long long), r_.altivec_f32),\n', '              HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(long long), vec_splat_s32(0))\n', '            )\n', '          );\n', '      #else\n', '        const SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) perm = {\n', '          0x00, 0x01, 0x02, 0x03, /* 0 */\n', '          0x08, 0x09, 0x0a, 0x0b, /* 2 */\n', '          0x10, 0x11, 0x12, 0x13, /* 4 */\n', '          0x18, 0x19, 0x1a, 0x1b  /* 6 */\n', '        };\n', '        r_.altivec_f32 = vec_perm(r_.altivec_f32, HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(float), vec_splat_s32(0)), perm);\n', '      #endif\n', '    #elif HEDLEY_HAS_BUILTIN(__builtin_shufflevector) && HEDLEY_HAS_BUILTIN(__builtin_convertvector)\n', '      float __attribute__((__vector_size__(8))) z = { 0.0f, 0.0f };\n', '      r_.f32 = __builtin_shufflevector(__builtin_convertvector(a_.f64, __typeof__(z)), z, 0, 1, 2, 3);\n', '    #else\n', '      r_.f32[0] = HEDLEY_STATIC_CAST(simde_float32, a_.f64[0]);\n', '      r_.f32[1] = HEDLEY_STATIC_CAST(simde_float32, a_.f64[1]);\n', '      r_.f32[2] = SIMDE_FLOAT32_C(0.0);\n', '      r_.f32[3] = SIMDE_FLOAT32_C(0.0);\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'name': 'simde_wasm_f32x4_demote_f64x2_zero',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_f32 = vec_floate(a_.altivec_f64);\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_demote_f64x2_zero(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_cvtpd_ps(a_.sse_m128d);\n']
}, {
    'name': 'simde_wasm_i16x8_extend_low_i8x16',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = HEDLEY_STATIC_CAST(int16_t, a_.i8[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const int8_t v SIMDE_VECTOR(8) = {\n', '        a_.i8[0], a_.i8[1], a_.i8[2], a_.i8[3],\n', '        a_.i8[4], a_.i8[5], a_.i8[6], a_.i8[7]\n', '      };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i16, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_extend_low_i8x16(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vmovl_s8(vget_low_s8(a_.neon_i8));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepi8_epi16(a_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_srai_epi16(_mm_unpacklo_epi8(a_.sse_m128i, a_.sse_m128i), 8);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 =\n', '        vec_sra(\n', '          HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(short), vec_mergeh(a_.altivec_i8, a_.altivec_i8)),\n', '          vec_splats(HEDLEY_STATIC_CAST(unsigned short, 8)\n', '        )\n', '      );\n']
}, {
    'name': 'simde_wasm_i32x4_extend_low_i16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, a_.i16[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const int16_t v SIMDE_VECTOR(8) = { a_.i16[0], a_.i16[1], a_.i16[2], a_.i16[3] };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i32, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_extend_low_i16x8(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vmovl_s16(vget_low_s16(a_.neon_i16));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepi16_epi32(a_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_srai_epi32(_mm_unpacklo_epi16(a_.sse_m128i, a_.sse_m128i), 16);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 =\n', '        vec_sra(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(int), vec_mergeh(a_.altivec_i16, a_.altivec_i16)),\n', '        vec_splats(HEDLEY_STATIC_CAST(unsigned int, 16))\n', '      );\n']
}, {
    'name': 'simde_wasm_i64x2_extend_low_i32x4',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_i64 =\n', '        vec_sra(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(long long), vec_mergeh(a_.altivec_i32, a_.altivec_i32)),\n', '        vec_splats(HEDLEY_STATIC_CAST(unsigned long long, 32))\n', '      );\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = HEDLEY_STATIC_CAST(int64_t, a_.i32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const int32_t v SIMDE_VECTOR(8) = { a_.i32[0], a_.i32[1] };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i64, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_extend_low_i32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i64 = vmovl_s32(vget_low_s32(a_.neon_i32));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepi32_epi64(a_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_unpacklo_epi32(a_.sse_m128i, _mm_cmpgt_epi32(_mm_setzero_si128(), a_.sse_m128i));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 =\n', '        vec_mergeh(\n', '          a_.altivec_i32,\n', '          HEDLEY_REINTERPRET_CAST(\n', '            SIMDE_POWER_ALTIVEC_VECTOR(int),\n', '            vec_cmpgt(vec_splat_s32(0), a_.altivec_i32)\n', '          )\n', '        );\n']
}, {
    'name': 'simde_wasm_u16x8_extend_low_u8x16',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = HEDLEY_STATIC_CAST(int16_t, a_.u8[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const uint8_t v SIMDE_VECTOR(8) = {\n', '        a_.u8[0], a_.u8[1], a_.u8[2], a_.u8[3],\n', '        a_.u8[4], a_.u8[5], a_.u8[6], a_.u8[7]\n', '      };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i16, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_extend_low_u8x16(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vmovl_u8(vget_low_u8(a_.neon_u8));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepu8_epi16(a_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_srli_epi16(_mm_unpacklo_epi8(a_.sse_m128i, a_.sse_m128i), 8);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = vec_mergeh(a_.altivec_i8, vec_splat_s8(0));\n']
}, {
    'name': 'simde_wasm_u32x4_extend_low_u16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, a_.u16[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const uint16_t v SIMDE_VECTOR(8) = { a_.u16[0], a_.u16[1], a_.u16[2], a_.u16[3] };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i32, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_extend_low_u16x8(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vmovl_u16(vget_low_u16(a_.neon_u16));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepu16_epi32(a_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_srli_epi32(_mm_unpacklo_epi16(a_.sse_m128i, a_.sse_m128i), 16);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = vec_mergeh(a_.altivec_i16, vec_splat_s16(0));\n']
}, {
    'name': 'simde_wasm_u64x2_extend_low_u32x4',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u64) / sizeof(r_.u64[0])) ; i++) {\n', '        r_.u64[i] = HEDLEY_STATIC_CAST(int64_t, a_.u32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const uint32_t v SIMDE_VECTOR(8) = { a_.u32[0], a_.u32[1] };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.u64, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u64x2_extend_low_u32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u64 = vmovl_u32(vget_low_u32(a_.neon_u32));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepu32_epi64(a_.sse_m128i);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =_mm_unpacklo_epi32(a_.sse_m128i, _mm_setzero_si128());\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 = vec_mergeh(a_.altivec_i32, vec_splat_s32(0));\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f64 = vcvt_f64_f32(vget_low_f32(a_.neon_f32));\n'],
    'name': 'simde_wasm_f64x2_promote_low_f32x4',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_f64 = vec_unpackh(a_.altivec_f32);\n', '    #elif HEDLEY_HAS_BUILTIN(__builtin_shufflevector) && HEDLEY_HAS_BUILTIN(__builtin_convertvector)\n', '      r_.f64 = __builtin_convertvector(__builtin_shufflevector(a_.f32, a_.f32, 0, 1), __typeof__(r_.f64));\n', '    #else\n', '      r_.f64[0] = HEDLEY_STATIC_CAST(simde_float64, a_.f32[0]);\n', '      r_.f64[1] = HEDLEY_STATIC_CAST(simde_float64, a_.f32[1]);\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_promote_low_f32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128d = _mm_cvtps_pd(a_.sse_m128);\n']
}, {
    'name': 'simde_wasm_i16x8_extend_high_i8x16',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = HEDLEY_STATIC_CAST(int16_t, a_.i8[i + 8]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const int8_t v SIMDE_VECTOR(8) = {\n', '        a_.i8[ 8], a_.i8[ 9], a_.i8[10], a_.i8[11],\n', '        a_.i8[12], a_.i8[13], a_.i8[14], a_.i8[15]\n', '      };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i16, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_extend_high_i8x16(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vmovl_s8(vget_high_s8(a_.neon_i8));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepi8_epi16(_mm_shuffle_epi32(a_.sse_m128i, _MM_SHUFFLE(3, 2, 3, 2)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_srai_epi16(_mm_unpackhi_epi8(a_.sse_m128i, a_.sse_m128i), 8);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 =\n', '        vec_sra(\n', '          HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(short), vec_mergel(a_.altivec_i8, a_.altivec_i8)),\n', '          vec_splats(HEDLEY_STATIC_CAST(unsigned short, 8)\n', '        )\n', '      );\n']
}, {
    'name': 'simde_wasm_i32x4_extend_high_i16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, a_.i16[i + 4]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const int16_t v SIMDE_VECTOR(8) = { a_.i16[4], a_.i16[5], a_.i16[6], a_.i16[7] };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i32, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_extend_high_i16x8(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vmovl_s16(vget_high_s16(a_.neon_i16));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepi16_epi32(_mm_shuffle_epi32(a_.sse_m128i, _MM_SHUFFLE(3, 2, 3, 2)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_srai_epi32(_mm_unpackhi_epi16(a_.sse_m128i, a_.sse_m128i), 16);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 =\n', '        vec_sra(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(int), vec_mergel(a_.altivec_i16, a_.altivec_i16)),\n', '        vec_splats(HEDLEY_STATIC_CAST(unsigned int, 16))\n', '      );\n']
}, {
    'name': 'simde_wasm_i64x2_extend_high_i32x4',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_i64 =\n', '        vec_sra(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(long long), vec_mergel(a_.altivec_i32, a_.altivec_i32)),\n', '        vec_splats(HEDLEY_STATIC_CAST(unsigned long long, 32))\n', '      );\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = HEDLEY_STATIC_CAST(int64_t, a_.i32[i + 2]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const int32_t v SIMDE_VECTOR(8) = { a_.i32[2], a_.i32[3] };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.i64, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_extend_high_i32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i64 = vmovl_s32(vget_high_s32(a_.neon_i32));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepi32_epi64(_mm_shuffle_epi32(a_.sse_m128i, _MM_SHUFFLE(3, 2, 3, 2)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_unpackhi_epi32(a_.sse_m128i, _mm_cmpgt_epi32(_mm_setzero_si128(), a_.sse_m128i));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 =\n', '        vec_mergel(\n', '          a_.altivec_i32,\n', '          HEDLEY_REINTERPRET_CAST(\n', '            SIMDE_POWER_ALTIVEC_VECTOR(int),\n', '            vec_cmpgt(vec_splat_s32(0), a_.altivec_i32)\n', '          )\n', '        );\n']
}, {
    'name': 'simde_wasm_u16x8_extend_high_u8x16',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = HEDLEY_STATIC_CAST(uint16_t, a_.u8[i + 8]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const uint8_t v SIMDE_VECTOR(8) = {\n', '        a_.u8[ 8], a_.u8[ 9], a_.u8[10], a_.u8[11],\n', '        a_.u8[12], a_.u8[13], a_.u8[14], a_.u8[15]\n', '      };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.u16, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_extend_high_u8x16(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vmovl_u8(vget_high_u8(a_.neon_u8));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepu8_epi16(_mm_shuffle_epi32(a_.sse_m128i, _MM_SHUFFLE(3, 2, 3, 2)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_srli_epi16(_mm_unpackhi_epi8(a_.sse_m128i, a_.sse_m128i), 8);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i8 = vec_mergel(a_.altivec_i8, vec_splat_s8(0));\n']
}, {
    'name': 'simde_wasm_u32x4_extend_high_u16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = HEDLEY_STATIC_CAST(uint32_t, a_.u16[i + 4]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const uint16_t v SIMDE_VECTOR(8) = { a_.u16[4], a_.u16[5], a_.u16[6], a_.u16[7] };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.u32, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_extend_high_u16x8(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vmovl_u16(vget_high_u16(a_.neon_u16));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepu16_epi32(_mm_shuffle_epi32(a_.sse_m128i, _MM_SHUFFLE(3, 2, 3, 2)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_srli_epi32(_mm_unpackhi_epi16(a_.sse_m128i, a_.sse_m128i), 16);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 = vec_mergel(a_.altivec_i16, vec_splat_s16(0));\n']
}, {
    'name': 'simde_wasm_u64x2_extend_high_u32x4',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = HEDLEY_STATIC_CAST(uint32_t, a_.u32[i + 2]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      const uint32_t v SIMDE_VECTOR(8) = { a_.u32[2], a_.u32[3] };\n', '\n', '      SIMDE_CONVERT_VECTOR_(r_.u64, v);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u64x2_extend_high_u32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u64 = vmovl_u32(vget_high_u32(a_.neon_u32));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i = _mm_cvtepu32_epi64(_mm_shuffle_epi32(a_.sse_m128i, _MM_SHUFFLE(3, 2, 3, 2)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =_mm_unpackhi_epi32(a_.sse_m128i, _mm_setzero_si128());\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 = vec_mergel(a_.altivec_i32, vec_splat_s32(0));\n']
}, {
    'powerpc_altivec_p8_mapping': ['      #if defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '        ashuf = vec_mergeh(a_.altivec_i8, a_.altivec_i8);\n', '        bshuf = vec_mergeh(b_.altivec_i8, b_.altivec_i8);\n', '      #else\n', '        SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) perm = {\n', '          0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7\n', '        };\n', '        ashuf = vec_perm(a_.altivec_i8, a_.altivec_i8, perm);\n', '        bshuf = vec_perm(b_.altivec_i8, b_.altivec_i8, perm);\n', '      #endif\n', '\n', '      r_.altivec_i16 = vec_mule(ashuf, bshuf);\n'],
    'name': 'simde_wasm_i16x8_extmul_low_i8x16',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = HEDLEY_STATIC_CAST(int16_t, a_.i8[i]) * HEDLEY_STATIC_CAST(int16_t, b_.i8[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.i16 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.i8, a_.i8, 0, 1, 2, 3, 4, 5, 6, 7),\n', '          __typeof__(r_.i16)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.i8, b_.i8, 0, 1, 2, 3, 4, 5, 6, 7),\n', '          __typeof__(r_.i16)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_extmul_low_i8x16(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vmull_s8(vget_low_s8(a_.neon_i8), vget_low_s8(b_.neon_i8));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_mullo_epi16(\n', '          _mm_srai_epi16(_mm_unpacklo_epi8(a_.sse_m128i, a_.sse_m128i), 8),\n', '          _mm_srai_epi16(_mm_unpacklo_epi8(b_.sse_m128i, b_.sse_m128i), 8)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed char) ashuf;\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed char) bshuf;\n', '\n']
}, {
    'powerpc_altivec_p8_mapping': ['      #if defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '        ashuf = vec_mergeh(a_.altivec_i16, a_.altivec_i16);\n', '        bshuf = vec_mergeh(b_.altivec_i16, b_.altivec_i16);\n', '      #else\n', '        SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) perm = {\n', '          0, 1, 0, 1,\n', '          2, 3, 2, 3,\n', '          4, 5, 4, 5,\n', '          6, 7, 6, 7\n', '        };\n', '        ashuf = vec_perm(a_.altivec_i16, a_.altivec_i16, perm);\n', '        bshuf = vec_perm(b_.altivec_i16, b_.altivec_i16, perm);\n', '      #endif\n', '\n', '      r_.altivec_i32 = vec_mule(ashuf, bshuf);\n'],
    'name': 'simde_wasm_i32x4_extmul_low_i16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, a_.i16[i]) * HEDLEY_STATIC_CAST(int32_t, b_.i16[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.i32 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.i16, a_.i16, 0, 1, 2, 3),\n', '          __typeof__(r_.i32)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.i16, b_.i16, 0, 1, 2, 3),\n', '          __typeof__(r_.i32)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_extmul_low_i16x8(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vmull_s16(vget_low_s16(a_.neon_i16), vget_low_s16(b_.neon_i16));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_unpacklo_epi16(\n', '          _mm_mullo_epi16(a_.sse_m128i, b_.sse_m128i),\n', '          _mm_mulhi_epi16(a_.sse_m128i, b_.sse_m128i)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed short) ashuf;\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed short) bshuf;\n', '\n']
}, {
    'powerpc_altivec_p8_mapping': ['      #if defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '        ashuf = vec_mergeh(a_.altivec_i32, a_.altivec_i32);\n', '        bshuf = vec_mergeh(b_.altivec_i32, b_.altivec_i32);\n', '      #else\n', '        SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) perm = {\n', '          0, 1, 2, 3, 0, 1, 2, 3,\n', '          4, 5, 6, 7, 4, 5, 6, 7\n', '        };\n', '        ashuf = vec_perm(a_.altivec_i32, a_.altivec_i32, perm);\n', '        bshuf = vec_perm(b_.altivec_i32, b_.altivec_i32, perm);\n', '      #endif\n', '\n', '      r_.altivec_i64 = vec_mule(ashuf, bshuf);\n'],
    'name': 'simde_wasm_i64x2_extmul_low_i32x4',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed int) ashuf;\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed int) bshuf;\n', '\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = HEDLEY_STATIC_CAST(int64_t, a_.i32[i]) * HEDLEY_STATIC_CAST(int64_t, b_.i32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.i64 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.i32, a_.i32, 0, 1),\n', '          __typeof__(r_.i64)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.i32, b_.i32, 0, 1),\n', '          __typeof__(r_.i64)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_extmul_low_i32x4(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i64 = vmull_s32(vget_low_s32(a_.neon_i32), vget_low_s32(b_.neon_i32));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_mul_epi32(\n', '          _mm_shuffle_epi32(a_.sse_m128i, _MM_SHUFFLE(1, 1, 0, 0)),\n', '          _mm_shuffle_epi32(b_.sse_m128i, _MM_SHUFFLE(1, 1, 0, 0))\n', '        );\n'],
    'type': 'simde_v128_t'
}, {
    'powerpc_altivec_p8_mapping': ['      #if defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '        ashuf = vec_mergeh(a_.altivec_u8, a_.altivec_u8);\n', '        bshuf = vec_mergeh(b_.altivec_u8, b_.altivec_u8);\n', '      #else\n', '        SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) perm = {\n', '          0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7\n', '        };\n', '        ashuf = vec_perm(a_.altivec_u8, a_.altivec_u8, perm);\n', '        bshuf = vec_perm(b_.altivec_u8, b_.altivec_u8, perm);\n', '      #endif\n', '\n', '      r_.altivec_u16 = vec_mule(ashuf, bshuf);\n'],
    'name': 'simde_wasm_u16x8_extmul_low_u8x16',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = HEDLEY_STATIC_CAST(uint16_t, a_.u8[i]) * HEDLEY_STATIC_CAST(uint16_t, b_.u8[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.u16 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.u8, a_.u8, 0, 1, 2, 3, 4, 5, 6, 7),\n', '          __typeof__(r_.u16)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.u8, b_.u8, 0, 1, 2, 3, 4, 5, 6, 7),\n', '          __typeof__(r_.u16)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_extmul_low_u8x16(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vmull_u8(vget_low_u8(a_.neon_u8), vget_low_u8(b_.neon_u8));\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) ashuf;\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) bshuf;\n', '\n']
}, {
    'powerpc_altivec_p8_mapping': ['      #if defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '        ashuf = vec_mergeh(a_.altivec_u16, a_.altivec_u16);\n', '        bshuf = vec_mergeh(b_.altivec_u16, b_.altivec_u16);\n', '      #else\n', '        SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) perm = {\n', '          0, 1, 0, 1,\n', '          2, 3, 2, 3,\n', '          4, 5, 4, 5,\n', '          6, 7, 6, 7\n', '        };\n', '        ashuf = vec_perm(a_.altivec_u16, a_.altivec_u16, perm);\n', '        bshuf = vec_perm(b_.altivec_u16, b_.altivec_u16, perm);\n', '      #endif\n', '\n', '      r_.altivec_u32 = vec_mule(ashuf, bshuf);\n'],
    'name': 'simde_wasm_u32x4_extmul_low_u16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u32) / sizeof(r_.u32[0])) ; i++) {\n', '        r_.u32[i] = HEDLEY_STATIC_CAST(uint32_t, a_.u16[i]) * HEDLEY_STATIC_CAST(uint32_t, b_.u16[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.u32 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.u16, a_.u16, 0, 1, 2, 3),\n', '          __typeof__(r_.u32)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.u16, b_.u16, 0, 1, 2, 3),\n', '          __typeof__(r_.u32)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_extmul_low_u16x8(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vmull_u16(vget_low_u16(a_.neon_u16), vget_low_u16(b_.neon_u16));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_unpacklo_epi16(\n', '          _mm_mullo_epi16(a_.sse_m128i, b_.sse_m128i),\n', '          _mm_mulhi_epu16(a_.sse_m128i, b_.sse_m128i)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned short) ashuf;\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned short) bshuf;\n', '\n']
}, {
    'powerpc_altivec_p8_mapping': ['      #if defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '        ashuf = vec_mergeh(a_.altivec_u32, a_.altivec_u32);\n', '        bshuf = vec_mergeh(b_.altivec_u32, b_.altivec_u32);\n', '      #else\n', '        SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) perm = {\n', '          0, 1, 2, 3, 0, 1, 2, 3,\n', '          4, 5, 6, 7, 4, 5, 6, 7\n', '        };\n', '        ashuf = vec_perm(a_.altivec_u32, a_.altivec_u32, perm);\n', '        bshuf = vec_perm(b_.altivec_u32, b_.altivec_u32, perm);\n', '      #endif\n', '\n', '      r_.altivec_u64 = vec_mule(ashuf, bshuf);\n'],
    'name': 'simde_wasm_u64x2_extmul_low_u32x4',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned int) ashuf;\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned int) bshuf;\n', '\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.u64[i] = HEDLEY_STATIC_CAST(uint64_t, a_.u32[i]) * HEDLEY_STATIC_CAST(uint64_t, b_.u32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.u64 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.u32, a_.u32, 0, 1),\n', '          __typeof__(r_.u64)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.u32, b_.u32, 0, 1),\n', '          __typeof__(r_.u64)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u64x2_extmul_low_u32x4(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u64 = vmull_u32(vget_low_u32(a_.neon_u32), vget_low_u32(b_.neon_u32));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_mul_epu32(\n', '          _mm_shuffle_epi32(a_.sse_m128i, _MM_SHUFFLE(1, 1, 0, 0)),\n', '          _mm_shuffle_epi32(b_.sse_m128i, _MM_SHUFFLE(1, 1, 0, 0))\n', '        );\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_i16 = vmull_high_s8(a_.neon_i8, b_.neon_i8);\n'],
    'name': 'simde_wasm_i16x8_extmul_high_i8x16',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = HEDLEY_STATIC_CAST(int16_t, a_.i8[i + 8]) * HEDLEY_STATIC_CAST(int16_t, b_.i8[i + 8]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.i16 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.i8, a_.i8, 8, 9, 10, 11, 12, 13, 14, 15),\n', '          __typeof__(r_.i16)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.i8, b_.i8, 8, 9, 10, 11, 12, 13, 14, 15),\n', '          __typeof__(r_.i16)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_extmul_high_i8x16(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vmull_s8(vget_high_s8(a_.neon_i8), vget_high_s8(b_.neon_i8));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_mullo_epi16(\n', '          _mm_srai_epi16(_mm_unpackhi_epi8(a_.sse_m128i, a_.sse_m128i), 8),\n', '          _mm_srai_epi16(_mm_unpackhi_epi8(b_.sse_m128i, b_.sse_m128i), 8)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i16 =\n', '        vec_mule(\n', '          vec_mergel(a_.altivec_i8, a_.altivec_i8),\n', '          vec_mergel(b_.altivec_i8, b_.altivec_i8)\n', '        );\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_i32 = vmull_high_s16(a_.neon_i16, b_.neon_i16);\n'],
    'name': 'simde_wasm_i32x4_extmul_high_i16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, a_.i16[i + 4]) * HEDLEY_STATIC_CAST(int32_t, b_.i16[i + 4]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.i32 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.i16, a_.i16, 4, 5, 6, 7),\n', '          __typeof__(r_.i32)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.i16, b_.i16, 4, 5, 6, 7),\n', '          __typeof__(r_.i32)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_extmul_high_i16x8(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vmull_s16(vget_high_s16(a_.neon_i16), vget_high_s16(b_.neon_i16));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_unpackhi_epi16(\n', '          _mm_mullo_epi16(a_.sse_m128i, b_.sse_m128i),\n', '          _mm_mulhi_epi16(a_.sse_m128i, b_.sse_m128i)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 =\n', '        vec_mule(\n', '          vec_mergel(a_.altivec_i16, a_.altivec_i16),\n', '          vec_mergel(b_.altivec_i16, b_.altivec_i16)\n', '        );\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_i64 = vmull_high_s32(a_.neon_i32, b_.neon_i32);\n'],
    'powerpc_altivec_p8_mapping': ['      #if defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '        ashuf = vec_mergel(a_.altivec_i32, a_.altivec_i32);\n', '        bshuf = vec_mergel(b_.altivec_i32, b_.altivec_i32);\n', '      #else\n', '        SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) perm = {\n', '           8,  9, 10, 11,  8,  9, 10, 11,\n', '          12, 13, 14, 15, 12, 13, 14, 15\n', '        };\n', '        ashuf = vec_perm(a_.altivec_i32, a_.altivec_i32, perm);\n', '        bshuf = vec_perm(b_.altivec_i32, b_.altivec_i32, perm);\n', '      #endif\n', '\n', '      r_.altivec_i64 = vec_mule(ashuf, bshuf);\n'],
    'name': 'simde_wasm_i64x2_extmul_high_i32x4',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed int) ashuf;\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed int) bshuf;\n', '\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = HEDLEY_STATIC_CAST(int64_t, a_.i32[i + 2]) * HEDLEY_STATIC_CAST(int64_t, b_.i32[i + 2]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.i64 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.i32, a_.i32, 2, 3),\n', '          __typeof__(r_.i64)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.i32, b_.i32, 2, 3),\n', '          __typeof__(r_.i64)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_extmul_high_i32x4(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i64 = vmull_s32(vget_high_s32(a_.neon_i32), vget_high_s32(b_.neon_i32));\n'],
    'x86_sse41_mapping': ['    #elif defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_mul_epi32(\n', '          _mm_shuffle_epi32(a_.sse_m128i, _MM_SHUFFLE(3, 3, 2, 2)),\n', '          _mm_shuffle_epi32(b_.sse_m128i, _MM_SHUFFLE(3, 3, 2, 2))\n', '        );\n'],
    'type': 'simde_v128_t'
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u16 = vmull_high_u8(a_.neon_u8, b_.neon_u8);\n'],
    'name': 'simde_wasm_u16x8_extmul_high_u8x16',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = HEDLEY_STATIC_CAST(uint16_t, a_.u8[i + 8]) * HEDLEY_STATIC_CAST(uint16_t, b_.u8[i + 8]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.u16 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.u8, a_.u8, 8, 9, 10, 11, 12, 13, 14, 15),\n', '          __typeof__(r_.u16)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.u8, b_.u8, 8, 9, 10, 11, 12, 13, 14, 15),\n', '          __typeof__(r_.u16)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_extmul_high_u8x16(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vmull_u8(vget_high_u8(a_.neon_u8), vget_high_u8(b_.neon_u8));\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u16 =\n', '        vec_mule(\n', '          vec_mergel(a_.altivec_u8, a_.altivec_u8),\n', '          vec_mergel(b_.altivec_u8, b_.altivec_u8)\n', '        );\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u32 = vmull_high_u16(a_.neon_u16, b_.neon_u16);\n'],
    'name': 'simde_wasm_u32x4_extmul_high_u16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u32) / sizeof(r_.u32[0])) ; i++) {\n', '        r_.u32[i] = HEDLEY_STATIC_CAST(uint32_t, a_.u16[i + 4]) * HEDLEY_STATIC_CAST(uint32_t, b_.u16[i + 4]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.u32 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.u16, a_.u16, 4, 5, 6, 7),\n', '          __typeof__(r_.u32)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.u16, b_.u16, 4, 5, 6, 7),\n', '          __typeof__(r_.u32)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_extmul_high_u16x8(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vmull_u16(vget_high_u16(a_.neon_u16), vget_high_u16(b_.neon_u16));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_unpackhi_epi16(\n', '          _mm_mullo_epi16(a_.sse_m128i, b_.sse_m128i),\n', '          _mm_mulhi_epu16(a_.sse_m128i, b_.sse_m128i)\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_u32 =\n', '        vec_mule(\n', '          vec_mergel(a_.altivec_u16, a_.altivec_u16),\n', '          vec_mergel(b_.altivec_u16, b_.altivec_u16)\n', '        );\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u64 = vmull_high_u32(a_.neon_u32, b_.neon_u32);\n'],
    'name': 'simde_wasm_u64x2_extmul_high_u32x4',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_u64 =\n', '        vec_mule(\n', '          vec_mergel(a_.altivec_u32, a_.altivec_u32),\n', '          vec_mergel(b_.altivec_u32, b_.altivec_u32)\n', '        );\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u64) / sizeof(r_.u64[0])) ; i++) {\n', '        r_.u64[i] = HEDLEY_STATIC_CAST(uint64_t, a_.u32[i + 2]) * HEDLEY_STATIC_CAST(uint64_t, b_.u32[i + 2]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      r_.u64 =\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(a_.u32, a_.u32, 2, 3),\n', '          __typeof__(r_.u64)\n', '        )\n', '        *\n', '        __builtin_convertvector(\n', '          __builtin_shufflevector(b_.u32, b_.u32, 2, 3),\n', '          __typeof__(r_.u64)\n', '        );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u64x2_extmul_high_u32x4(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u64 = vmull_u32(vget_high_u32(a_.neon_u32), vget_high_u32(b_.neon_u32));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_mul_epu32(\n', '          _mm_shuffle_epi32(a_.sse_m128i, _MM_SHUFFLE(3, 3, 2, 2)),\n', '          _mm_shuffle_epi32(b_.sse_m128i, _MM_SHUFFLE(3, 3, 2, 2))\n', '        );\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = HEDLEY_STATIC_CAST(int16_t, a_.i8[(i * 2)]) + HEDLEY_STATIC_CAST(int16_t, a_.i8[(i * 2) + 1]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'name': 'simde_wasm_i16x8_extadd_pairwise_i8x16',
    'x86_ssse3_mapping': ['    #elif defined(SIMDE_X86_SSSE3_NATIVE)\n', '      r_.sse_m128i = _mm_maddubs_epi16(_mm_set1_epi8(INT8_C(1)), a_.sse_m128i);\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      r_.i16 =\n', '        ((a_.i16 << 8) >> 8) +\n', '        ((a_.i16 >> 8)     );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_extadd_pairwise_i8x16(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'x86_xop_mapping': ['    #elif defined(SIMDE_X86_XOP_NATIVE)\n', '      r_.sse_m128i = _mm_haddw_epi8(a_.sse_m128i);\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i16 = vpaddlq_s8(a_.neon_i8);\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed char) one = vec_splat_s8(1);\n', '      r_.altivec_i16 =\n', '        vec_add(\n', '          vec_mule(a_.altivec_i8, one),\n', '          vec_mulo(a_.altivec_i8, one)\n', '        );\n']
}, {
    'name': 'simde_wasm_i32x4_extadd_pairwise_i16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, a_.i16[(i * 2)]) + HEDLEY_STATIC_CAST(int32_t, a_.i16[(i * 2) + 1]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      r_.i32 =\n', '        ((a_.i32 << 16) >> 16) +\n', '        ((a_.i32 >> 16)      );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_extadd_pairwise_i16x8(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'x86_xop_mapping': ['    #elif defined(SIMDE_X86_XOP_NATIVE)\n', '      r_.sse_m128i = _mm_haddd_epi16(a_.sse_m128i);\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vpaddlq_s16(a_.neon_i16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_madd_epi16(a_.sse_m128i, _mm_set1_epi16(INT8_C(1)));\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(signed short) one = vec_splat_s16(1);\n', '      r_.altivec_i32 =\n', '        vec_add(\n', '          vec_mule(a_.altivec_i16, one),\n', '          vec_mulo(a_.altivec_i16, one)\n', '        );\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = HEDLEY_STATIC_CAST(uint16_t, a_.u8[(i * 2)]) + HEDLEY_STATIC_CAST(uint16_t, a_.u8[(i * 2) + 1]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'name': 'simde_wasm_u16x8_extadd_pairwise_u8x16',
    'x86_ssse3_mapping': ['    #elif defined(SIMDE_X86_SSSE3_NATIVE)\n', '      r_.sse_m128i = _mm_maddubs_epi16(a_.sse_m128i, _mm_set1_epi8(INT8_C(1)));\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      r_.u16 =\n', '        ((a_.u16 << 8) >> 8) +\n', '        ((a_.u16 >> 8)     );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_extadd_pairwise_u8x16(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'x86_xop_mapping': ['    #elif defined(SIMDE_X86_XOP_NATIVE)\n', '      r_.sse_m128i = _mm_haddw_epu8(a_.sse_m128i);\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u16 = vpaddlq_u8(a_.neon_u8);\n'],
    'type': 'simde_v128_t',
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) one = vec_splat_u8(1);\n', '      r_.altivec_u16 =\n', '        vec_add(\n', '          vec_mule(a_.altivec_u8, one),\n', '          vec_mulo(a_.altivec_u8, one)\n', '        );\n']
}, {
    'name': 'simde_wasm_u32x4_extadd_pairwise_u16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u32) / sizeof(r_.u32[0])) ; i++) {\n', '        r_.u32[i] = HEDLEY_STATIC_CAST(uint32_t, a_.u16[(i * 2)]) + HEDLEY_STATIC_CAST(uint32_t, a_.u16[(i * 2) + 1]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_SCALAR)\n', '      r_.u32 =\n', '        ((a_.u32 << 16) >> 16) +\n', '        ((a_.u32 >> 16)      );\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_extadd_pairwise_u16x8(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'x86_xop_mapping': ['    #elif defined(SIMDE_X86_XOP_NATIVE)\n', '      r_.sse_m128i = _mm_haddd_epu16(a_.sse_m128i);\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vpaddlq_u16(a_.neon_u16);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i =\n', '        _mm_add_epi32(\n', '          _mm_srli_epi32(a_.sse_m128i, 16),\n', '          _mm_and_si128(a_.sse_m128i, _mm_set1_epi32(INT32_C(0x0000ffff)))\n', '        );\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(unsigned short) one = vec_splat_u16(1);\n', '      r_.altivec_u32 =\n', '        vec_add(\n', '          vec_mule(a_.altivec_u16, one),\n', '          vec_mulo(a_.altivec_u16, one)\n', '        );\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i16) / sizeof(r_.i16[0])) ; i++) {\n', '        r_.i16[i] = HEDLEY_STATIC_CAST(int16_t, v[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i16x8_load8x8(mem);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i16x8_load8x8',
    'simde_convert_vector_mapping': ['    #if defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      int8_t v SIMDE_VECTOR(8);\n', '      simde_memcpy(&v, mem, sizeof(v));\n', '      SIMDE_CONVERT_VECTOR_(r_.i16, v);\n', '    #else\n', '      SIMDE_ALIGN_TO_16 int8_t v[8];\n', '      simde_memcpy(v, mem, sizeof(v));\n', '\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, v[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_load16x4(mem);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i32x4_load16x4',
    'simde_convert_vector_mapping': ['    #if defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      int16_t v SIMDE_VECTOR(8);\n', '      simde_memcpy(&v, mem, sizeof(v));\n', '      SIMDE_CONVERT_VECTOR_(r_.i32, v);\n', '    #else\n', '      SIMDE_ALIGN_TO_16 int16_t v[4];\n', '      simde_memcpy(v, mem, sizeof(v));\n', '\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i64) / sizeof(r_.i64[0])) ; i++) {\n', '        r_.i64[i] = HEDLEY_STATIC_CAST(int64_t, v[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i64x2_load32x2(mem);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_i64x2_load32x2',
    'simde_convert_vector_mapping': ['    #if defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762) && !defined(SIMDE_BUG_CLANG_50893)\n', '      int32_t v SIMDE_VECTOR(8);\n', '      simde_memcpy(&v, mem, sizeof(v));\n', '      SIMDE_CONVERT_VECTOR_(r_.i64, v);\n', '    #else\n', '      SIMDE_ALIGN_TO_16 int32_t v[2];\n', '      simde_memcpy(v, mem, sizeof(v));\n', '\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u16) / sizeof(r_.u16[0])) ; i++) {\n', '        r_.u16[i] = HEDLEY_STATIC_CAST(uint16_t, v[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u16x8_load8x8(mem);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_u16x8_load8x8',
    'simde_convert_vector_mapping': ['    #if defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      uint8_t v SIMDE_VECTOR(8);\n', '      simde_memcpy(&v, mem, sizeof(v));\n', '      SIMDE_CONVERT_VECTOR_(r_.u16, v);\n', '    #else\n', '      SIMDE_ALIGN_TO_16 uint8_t v[8];\n', '      simde_memcpy(v, mem, sizeof(v));\n', '\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u32) / sizeof(r_.u32[0])) ; i++) {\n', '        r_.u32[i] = HEDLEY_STATIC_CAST(uint32_t, v[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_load16x4(mem);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_u32x4_load16x4',
    'simde_convert_vector_mapping': ['    #if defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      uint16_t v SIMDE_VECTOR(8);\n', '      simde_memcpy(&v, mem, sizeof(v));\n', '      SIMDE_CONVERT_VECTOR_(r_.u32, v);\n', '    #else\n', '      SIMDE_ALIGN_TO_16 uint16_t v[4];\n', '      simde_memcpy(v, mem, sizeof(v));\n', '\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u64) / sizeof(r_.u64[0])) ; i++) {\n', '        r_.u64[i] = HEDLEY_STATIC_CAST(uint64_t, v[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u64x2_load32x2(mem);\n', '  #else\n', '    simde_v128_private r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_u64x2_load32x2',
    'simde_convert_vector_mapping': ['    #if defined(SIMDE_CONVERT_VECTOR_) && !defined(SIMDE_BUG_GCC_100762)\n', '      uint32_t v SIMDE_VECTOR(8);\n', '      simde_memcpy(&v, mem, sizeof(v));\n', '      SIMDE_CONVERT_VECTOR_(r_.u64, v);\n', '    #else\n', '      SIMDE_ALIGN_TO_16 uint32_t v[2];\n', '      simde_memcpy(v, mem, sizeof(v));\n', '\n']
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_load32_zero(a);\n', '  #else\n', '    simde_v128_private r_;\n', '\n', '    int32_t a_;\n', '    simde_memcpy(&a_, a, sizeof(a_));\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load32_zero',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_cvtsi32_si128(a_);\n', '    #else\n', '      r_.i32[0] = a_;\n', '      r_.i32[1] = 0;\n', '      r_.i32[2] = 0;\n', '      r_.i32[3] = 0;\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n']
}, {
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_v128_load64_zero(a);\n', '  #else\n', '    simde_v128_private r_;\n', '\n', '    int64_t a_;\n', '    simde_memcpy(&a_, a, sizeof(a_));\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load64_zero',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE) && defined(SIMDE_ARCH_AMD64)\n', '      r_.sse_m128i = _mm_cvtsi64_si128(a_);\n', '    #else\n', '      r_.i64[0] = a_;\n', '      r_.i64[1] = 0;\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_v128_load8_lane(a, vec, lane) wasm_v128_load8_lane(HEDLEY_CONST_CAST(int8_t *, (a)), (vec), (lane))\n', '#endif\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load8_lane',
    'tbd': ['    SIMDE_REQUIRE_CONSTANT_RANGE(lane, 0, 15) {\n', '  simde_v128_private\n', '    a_ = simde_v128_to_private(vec);\n', '\n', '  #if defined(SIMDE_BUG_CLANG_50901)\n', '    simde_v128_private r_ = simde_v128_to_private(vec);\n', '    r_.altivec_i8 = vec_insert(*HEDLEY_REINTERPRET_CAST(const signed char *, a), a_.altivec_i8, lane);\n', '    return simde_v128_from_private(r_);\n', '  #else\n', '    a_.i8[lane] = *HEDLEY_REINTERPRET_CAST(const int8_t *, a);\n', '    return simde_v128_from_private(a_);\n', '  #endif\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_v128_load16_lane(a, vec, lane) wasm_v128_load16_lane(HEDLEY_CONST_CAST(int16_t *, (a)), (vec), (lane))\n', '#endif\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load16_lane',
    'tbd': ['    SIMDE_REQUIRE_CONSTANT_RANGE(lane, 0, 7) {\n', '  simde_v128_private\n', '    a_ = simde_v128_to_private(vec);\n', '\n', '  a_.i16[lane] = *HEDLEY_REINTERPRET_CAST(const int16_t *, a);\n', '\n', '  return simde_v128_from_private(a_);\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_v128_load32_lane(a, vec, lane) wasm_v128_load32_lane(HEDLEY_CONST_CAST(int32_t *, (a)), (vec), (lane))\n', '#endif\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load32_lane',
    'tbd': ['    SIMDE_REQUIRE_CONSTANT_RANGE(lane, 0, 3) {\n', '  simde_v128_private\n', '    a_ = simde_v128_to_private(vec);\n', '\n', '  a_.i32[lane] = *HEDLEY_REINTERPRET_CAST(const int32_t *, a);\n', '\n', '  return simde_v128_from_private(a_);\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_v128_load64_lane(a, vec, lane) wasm_v128_load64_lane(HEDLEY_CONST_CAST(int64_t *, (a)), (vec), (lane))\n', '#endif\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_v128_load64_lane',
    'tbd': ['    SIMDE_REQUIRE_CONSTANT_RANGE(lane, 0, 1) {\n', '  simde_v128_private\n', '    a_ = simde_v128_to_private(vec);\n', '\n', '  a_.i64[lane] = *HEDLEY_REINTERPRET_CAST(const int64_t *, a);\n', '\n', '  return simde_v128_from_private(a_);\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_v128_store8_lane(a, vec, lane) wasm_v128_store8_lane((a), (vec), (lane))\n', '#endif\n'],
    'type': 'void',
    'name': 'simde_wasm_v128_store8_lane',
    'tbd': ['    SIMDE_REQUIRE_CONSTANT_RANGE(lane, 0, 15) {\n', '  simde_v128_private\n', '    vec_ = simde_v128_to_private(vec);\n', '\n', '  int8_t tmp = vec_.i8[lane];\n', '  simde_memcpy(a, &tmp, sizeof(tmp));\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_v128_store16_lane(a, vec, lane) wasm_v128_store16_lane((a), (vec), (lane))\n', '#endif\n'],
    'type': 'void',
    'name': 'simde_wasm_v128_store16_lane',
    'tbd': ['    SIMDE_REQUIRE_CONSTANT_RANGE(lane, 0, 7) {\n', '  simde_v128_private\n', '    vec_ = simde_v128_to_private(vec);\n', '\n', '  int16_t tmp = vec_.i16[lane];\n', '  simde_memcpy(a, &tmp, sizeof(tmp));\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_v128_store32_lane(a, vec, lane) wasm_v128_store32_lane((a), (vec), (lane))\n', '#endif\n'],
    'type': 'void',
    'name': 'simde_wasm_v128_store32_lane',
    'tbd': ['    SIMDE_REQUIRE_CONSTANT_RANGE(lane, 0, 3) {\n', '  simde_v128_private\n', '    vec_ = simde_v128_to_private(vec);\n', '\n', '  int32_t tmp = vec_.i32[lane];\n', '  simde_memcpy(a, &tmp, sizeof(tmp));\n', '}\n']
}, {
    'wasm': ['#if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '  #define simde_wasm_v128_store64_lane(a, vec, lane) wasm_v128_store64_lane((a), (vec), (lane))\n', '#endif\n'],
    'type': 'void',
    'name': 'simde_wasm_v128_store64_lane',
    'tbd': ['    SIMDE_REQUIRE_CONSTANT_RANGE(lane, 0, 1) {\n', '  simde_v128_private\n', '    vec_ = simde_v128_to_private(vec);\n', '\n', '  int64_t tmp = vec_.i64[lane];\n', '  simde_memcpy(a, &tmp, sizeof(tmp));\n', '}\n']
}, {
    'arm_neon_a32v7_nn_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7)\n', '      r_.neon_f32 = vcvtq_f32_s32(a_.neon_i32);\n'],
    'name': 'simde_wasm_f32x4_convert_i32x4',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = HEDLEY_STATIC_CAST(simde_float32, a_.i32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_)\n', '      SIMDE_CONVERT_VECTOR_(r_.f32, a_.i32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_convert_i32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128 = _mm_cvtepi32_ps(a_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      HEDLEY_DIAGNOSTIC_PUSH\n', '      #if HEDLEY_HAS_WARNING("-Wc11-extensions")\n', '        #pragma clang diagnostic ignored "-Wc11-extensions"\n', '      #endif\n', '      r_.altivec_f32 = vec_ctf(a_.altivec_i32, 0);\n', '      HEDLEY_DIAGNOSTIC_POP\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = HEDLEY_STATIC_CAST(simde_float32, a_.u32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_convert_u32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f32x4_convert_u32x4',
    'simde_convert_vector_mapping': ['    #if defined(SIMDE_CONVERT_VECTOR_)\n', '      SIMDE_CONVERT_VECTOR_(r_.f32, a_.u32);\n', '    #else\n']
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = HEDLEY_STATIC_CAST(simde_float64, a_.i32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_convert_low_i32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n', '    #if HEDLEY_HAS_BUILTIN(__builtin_shufflevector) && HEDLEY_HAS_BUILTIN(__builtin_convertvector)\n', '      r_.f64 = __builtin_convertvector(__builtin_shufflevector(a_.i32, a_.i32, 0, 1), __typeof__(r_.f64));\n', '    #else\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f64x2_convert_low_i32x4'
}, {
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = HEDLEY_STATIC_CAST(simde_float64, a_.u32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_convert_low_u32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n', '    #if HEDLEY_HAS_BUILTIN(__builtin_shufflevector) && HEDLEY_HAS_BUILTIN(__builtin_convertvector)\n', '      r_.f64 = __builtin_convertvector(__builtin_shufflevector(a_.u32, a_.u32, 0, 1), __typeof__(r_.f64));\n', '    #else\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f64x2_convert_low_u32x4'
}, {
    'name': 'simde_wasm_i32x4_trunc_sat_f32x4',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.i32) / sizeof(r_.i32[0])) ; i++) {\n', '        if (simde_math_isnanf(a_.f32[i])) {\n', '          r_.i32[i] = INT32_C(0);\n', '        } else if (a_.f32[i] < HEDLEY_STATIC_CAST(simde_float32, INT32_MIN)) {\n', '          r_.i32[i] = INT32_MIN;\n', '        } else if (a_.f32[i] > HEDLEY_STATIC_CAST(simde_float32, INT32_MAX)) {\n', '          r_.i32[i] = INT32_MAX;\n', '        } else {\n', '          r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, a_.f32[i]);\n', '        }\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && defined(SIMDE_FAST_CONVERSION_RANGE)\n', '      SIMDE_CONVERT_VECTOR_(r_.f32, a_.f32);\n', '    #elif defined(SIMDE_CONVERT_VECTOR_) && defined(SIMDE_IEEE754_STORAGE)\n', '      SIMDE_CONVERT_VECTOR_(r_.i32, a_.f32);\n', '\n', '      const __typeof__(a_.f32) max_representable = { SIMDE_FLOAT32_C(2147483520.0), SIMDE_FLOAT32_C(2147483520.0), SIMDE_FLOAT32_C(2147483520.0), SIMDE_FLOAT32_C(2147483520.0) };\n', '      __typeof__(r_.i32) max_mask = HEDLEY_REINTERPRET_CAST(__typeof__(max_mask), a_.f32 > max_representable);\n', '      __typeof__(r_.i32) max_i32 = { INT32_MAX, INT32_MAX, INT32_MAX, INT32_MAX };\n', '      r_.i32  = (max_i32 & max_mask) | (r_.i32 & ~max_mask);\n', '\n', '      const __typeof__(a_.f32) min_representable = { HEDLEY_STATIC_CAST(simde_float32, INT32_MIN), HEDLEY_STATIC_CAST(simde_float32, INT32_MIN), HEDLEY_STATIC_CAST(simde_float32, INT32_MIN), HEDLEY_STATIC_CAST(simde_float32, INT32_MIN) };\n', '      __typeof__(r_.i32) min_mask = HEDLEY_REINTERPRET_CAST(__typeof__(min_mask), a_.f32 < min_representable);\n', '      __typeof__(r_.i32) min_i32 = { INT32_MIN, INT32_MIN, INT32_MIN, INT32_MIN };\n', '      r_.i32  = (min_i32 & min_mask) | (r_.i32 & ~min_mask);\n', '\n', '      r_.i32 &= HEDLEY_REINTERPRET_CAST(__typeof__(r_.i32), a_.f32 == a_.f32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_trunc_sat_f32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i32 = vcvtq_s32_f32(a_.neon_f32);\n'],
    'x86_sse41_mapping': ['      #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '        r_.sse_m128i =\n', '          _mm_castps_si128(\n', '            _mm_blendv_ps(\n', '              _mm_castsi128_ps(r_.sse_m128i),\n', '              _mm_castsi128_ps(_mm_set1_epi32(INT32_MAX)),\n', '              _mm_castsi128_ps(i32_max_mask)\n', '            )\n', '          );\n', '      #else\n', '        r_.sse_m128i =\n', '          _mm_or_si128(\n', '            _mm_and_si128(i32_max_mask, _mm_set1_epi32(INT32_MAX)),\n', '            _mm_andnot_si128(i32_max_mask, r_.sse_m128i)\n', '          );\n', '      #endif\n', '      r_.sse_m128i = _mm_and_si128(r_.sse_m128i, _mm_castps_si128(_mm_cmpord_ps(a_.sse_m128, a_.sse_m128)));\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      const __m128i i32_max_mask = _mm_castps_si128(_mm_cmpgt_ps(a_.sse_m128, _mm_set1_ps(SIMDE_FLOAT32_C(2147483520.0))));\n', '      const __m128 clamped = _mm_max_ps(a_.sse_m128, _mm_set1_ps(HEDLEY_STATIC_CAST(simde_float32, INT32_MIN)));\n', '      r_.sse_m128i = _mm_cvttps_epi32(clamped);\n']
}, {
    'name': 'simde_wasm_u32x4_trunc_sat_f32x4',
    'x86_avx512vl_mapping': ['      #if defined(SIMDE_X86_AVX512VL_NATIVE)\n', '        r_.sse_m128i = _mm_cvttps_epu32(a_.sse_m128);\n', '      #else\n', '        __m128 first_oob_high = _mm_set1_ps(SIMDE_FLOAT32_C(4294967296.0));\n', '        __m128 neg_zero_if_too_high =\n', '          _mm_castsi128_ps(\n', '            _mm_slli_epi32(\n', '              _mm_castps_si128(_mm_cmple_ps(first_oob_high, a_.sse_m128)),\n', '              31\n', '            )\n', '          );\n', '        r_.sse_m128i =\n', '          _mm_xor_si128(\n', '            _mm_cvttps_epi32(\n', '              _mm_sub_ps(a_.sse_m128, _mm_and_ps(neg_zero_if_too_high, first_oob_high))\n', '            ),\n', '            _mm_castps_si128(neg_zero_if_too_high)\n', '          );\n', '      #endif\n', '\n', '      #if !defined(SIMDE_FAST_CONVERSION_RANGE)\n', '        r_.sse_m128i = _mm_and_si128(r_.sse_m128i, _mm_castps_si128(_mm_cmpgt_ps(a_.sse_m128, _mm_set1_ps(SIMDE_FLOAT32_C(0.0)))));\n', '        r_.sse_m128i = _mm_or_si128 (r_.sse_m128i, _mm_castps_si128(_mm_cmpge_ps(a_.sse_m128, _mm_set1_ps(SIMDE_FLOAT32_C(4294967296.0)))));\n', '      #endif\n', '\n', '      #if !defined(SIMDE_FAST_NANS)\n', '        r_.sse_m128i = _mm_and_si128(r_.sse_m128i, _mm_castps_si128(_mm_cmpord_ps(a_.sse_m128, a_.sse_m128)));\n', '      #endif\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u32) / sizeof(r_.u32[0])) ; i++) {\n', '        if (simde_math_isnan(a_.f32[i]) ||\n', '            a_.f32[i] < SIMDE_FLOAT32_C(0.0)) {\n', '          r_.u32[i] = UINT32_C(0);\n', '        } else if (a_.f32[i] > HEDLEY_STATIC_CAST(simde_float32, UINT32_MAX)) {\n', '          r_.u32[i] = UINT32_MAX;\n', '        } else {\n', '          r_.u32[i] = HEDLEY_STATIC_CAST(uint32_t, a_.f32[i]);\n', '        }\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_convert_vector_mapping': ['    #elif defined(SIMDE_CONVERT_VECTOR_) && defined(SIMDE_IEEE754_STORAGE)\n', '      SIMDE_CONVERT_VECTOR_(r_.u32, a_.f32);\n', '\n', '      const __typeof__(a_.f32) max_representable = { SIMDE_FLOAT32_C(4294967040.0), SIMDE_FLOAT32_C(4294967040.0), SIMDE_FLOAT32_C(4294967040.0), SIMDE_FLOAT32_C(4294967040.0) };\n', '      r_.u32 |= HEDLEY_REINTERPRET_CAST(__typeof__(r_.u32), a_.f32 > max_representable);\n', '\n', '      const __typeof__(a_.f32) min_representable = { SIMDE_FLOAT32_C(0.0), };\n', '      r_.u32 &= HEDLEY_REINTERPRET_CAST(__typeof__(r_.u32), a_.f32 > min_representable);\n', '\n', '      r_.u32 &= HEDLEY_REINTERPRET_CAST(__typeof__(r_.u32), a_.f32 == a_.f32);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_trunc_sat_f32x4(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_u32 = vcvtq_u32_f32(a_.neon_f32);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n']
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_i32 = vcombine_s32(vqmovn_s64(vcvtq_s64_f64(a_.neon_f64)), vdup_n_s32(INT32_C(0)));\n'],
    'powerpc_altivec_p8_mapping': ['      #if defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '        r_.altivec_i32 =\n', '          vec_pack(\n', '            HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(long long), r_.altivec_i32),\n', '            HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(long long), vec_splat_s32(0))\n', '          );\n', '      #else\n', '        SIMDE_POWER_ALTIVEC_VECTOR(unsigned char) perm = {\n', '           0,  1,  2,  3,  4,  5,  6,  7,\n', '          16, 17, 18, 19, 20, 21, 22, 23\n', '        };\n', '        r_.altivec_i32 =\n', '          HEDLEY_REINTERPRET_CAST(\n', '            SIMDE_POWER_ALTIVEC_VECTOR(signed int),\n', '            vec_perm(\n', '              HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed char), r_.altivec_i32),\n', '              vec_splat_s8(0),\n', '              perm\n', '            )\n', '          );\n', '      #endif\n', '    #else\n'],
    'name': 'simde_wasm_i32x4_trunc_sat_f64x2_zero',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      SIMDE_POWER_ALTIVEC_VECTOR(double) in_not_nan =\n', '        vec_and(a_.altivec_f64, vec_cmpeq(a_.altivec_f64, a_.altivec_f64));\n', '      r_.altivec_i32 = vec_signede(in_not_nan);\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(a_.f64) / sizeof(a_.f64[0])) ; i++) {\n', '        if (simde_math_isnan(a_.f64[i])) {\n', '          r_.i32[i] = INT32_C(0);\n', '        } else if (a_.f64[i] < HEDLEY_STATIC_CAST(simde_float64, INT32_MIN)) {\n', '          r_.i32[i] = INT32_MIN;\n', '        } else if (a_.f64[i] > HEDLEY_STATIC_CAST(simde_float64, INT32_MAX)) {\n', '          r_.i32[i] = INT32_MAX;\n', '        } else {\n', '          r_.i32[i] = HEDLEY_STATIC_CAST(int32_t, a_.f64[i]);\n', '        }\n', '      }\n', '      r_.i32[2] = 0;\n', '      r_.i32[3] = 0;\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_trunc_sat_f64x2_zero(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t'
}, {
    'arm_neon_a64v8_mapping': ['    #if defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_u32 = vcombine_u32(vqmovn_u64(vcvtq_u64_f64(a_.neon_f64)), vdup_n_u32(UINT32_C(0)));\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_u32x4_trunc_sat_f64x2_zero(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_u32x4_trunc_sat_f64x2_zero',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(a_.f64) / sizeof(a_.f64[0])) ; i++) {\n', '        if (simde_math_isnanf(a_.f64[i]) ||\n', '            a_.f64[i] < SIMDE_FLOAT64_C(0.0)) {\n', '          r_.u32[i] = UINT32_C(0);\n', '        } else if (a_.f64[i] > HEDLEY_STATIC_CAST(simde_float64, UINT32_MAX)) {\n', '          r_.u32[i] = UINT32_MAX;\n', '        } else {\n', '          r_.u32[i] = HEDLEY_STATIC_CAST(uint32_t, a_.f64[i]);\n', '        }\n', '      }\n', '      r_.u32[2] = 0;\n', '      r_.u32[3] = 0;\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n']
}, {
    'powerpc_altivec_p8_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P8_NATIVE)\n', '      r_.altivec_i8 = HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(signed char), vec_popcnt(HEDLEY_REINTERPRET_CAST(SIMDE_POWER_ALTIVEC_VECTOR(unsigned char), a_.altivec_i8)));\n', '    #else\n'],
    'name': 'simde_wasm_i8x16_popcnt',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.u8) / sizeof(r_.u8[0])) ; i++) {\n', '        uint8_t v = HEDLEY_STATIC_CAST(uint8_t, a_.u8[i]);\n', '        v = v - ((v >> 1) & (85));\n', '        v = (v & (51)) + ((v >> (2)) & (51));\n', '        v = (v + (v >> (4))) & (15);\n', '        r_.u8[i] = v >> (sizeof(uint8_t) - 1) * CHAR_BIT;\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'x86_avx512vl_mapping': ['    #elif defined(SIMDE_X86_AVX512VL_NATIVE) && defined(SIMDE_X86_AVX512BITALG_NATIVE)\n', '      r_.sse_m128i = _mm_popcnt_epi8(a_.sse_m128i);\n'],
    'x86_ssse3_mapping': ['    #elif defined(SIMDE_X86_SSSE3_NATIVE)\n', '      __m128i tmp0 = _mm_set1_epi8(0x0f);\n', '      __m128i tmp1 = _mm_and_si128(a_.sse_m128i, tmp0);\n', '      tmp0 = _mm_andnot_si128(tmp0, a_.sse_m128i);\n', '      __m128i y = _mm_set_epi8(4, 3, 3, 2, 3, 2, 2, 1, 3, 2, 2, 1, 2, 1, 1, 0);\n', '      tmp0 = _mm_srli_epi16(tmp0, 4);\n', '      y = _mm_shuffle_epi8(y, tmp1);\n', '      tmp1 = _mm_set_epi8(4, 3, 3, 2, 3, 2, 2, 1, 3, 2, 2, 1, 2, 1, 1, 0);\n', '      tmp1 = _mm_shuffle_epi8(tmp1, tmp0);\n', '      return _mm_add_epi8(y, tmp1);\n'],
    'x86_avx2_mapping': ['    #elif defined(SIMDE_X86_AVX2_NATIVE)\n', '      __m128i tmp0 = _mm_set1_epi8(0x0f);\n', '      __m128i tmp1 = _mm_andnot_si128(tmp0, a_.sse_m128i);\n', '      __m128i y = _mm_and_si128(tmp0, a_.sse_m128i);\n', '      tmp0 = _mm_set_epi8(4, 3, 3, 2, 3, 2, 2, 1, 3, 2, 2, 1, 2, 1, 1, 0);\n', '      tmp1 = _mm_srli_epi16(tmp1, 4);\n', '      y = _mm_shuffle_epi8(tmp0, y);\n', '      tmp1 = _mm_shuffle_epi8(tmp0, tmp1);\n', '      return _mm_add_epi8(y, tmp1);\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i8x16_popcnt(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #if defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      r_.neon_i8 = vcntq_s8(a_.neon_i8);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      __m128i tmp0 = _mm_and_si128(_mm_srli_epi16(a_.sse_m128i, 1), _mm_set1_epi8(0x55));\n', '      __m128i tmp1 = _mm_sub_epi8(a_.sse_m128i, tmp0);\n', '      tmp0 = tmp1;\n', '      tmp1 = _mm_and_si128(tmp1, _mm_set1_epi8(0x33));\n', '      tmp0 = _mm_and_si128(_mm_srli_epi16(tmp0, 2), _mm_set1_epi8(0x33));\n', '      tmp1 = _mm_add_epi8(tmp1, tmp0);\n', '      tmp0 = _mm_srli_epi16(tmp1, 4);\n', '      tmp1 = _mm_add_epi8(tmp1, tmp0);\n', '      r_.sse_m128i = _mm_and_si128(tmp1, _mm_set1_epi8(0x0f));\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      int32x4_t pl = vmull_s16(vget_low_s16(a_.neon_i16),  vget_low_s16(b_.neon_i16));\n', '      int32x4_t ph = vmull_high_s16(a_.neon_i16, b_.neon_i16);\n', '      r_.neon_i32 = vpaddq_s32(pl, ph);\n'],
    'name': 'simde_wasm_i32x4_dot_i16x8',
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_) / sizeof(r_.i16[0])) ; i += 2) {\n', '        r_.i32[i / 2] = (a_.i16[i] * b_.i16[i]) + (a_.i16[i + 1] * b_.i16[i + 1]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'simde_vector_subscript_mapping': ['    #elif defined(SIMDE_VECTOR_SUBSCRIPT_OPS) && defined(SIMDE_CONVERT_VECTOR_) && HEDLEY_HAS_BUILTIN(__builtin_shufflevector)\n', '      int32_t SIMDE_VECTOR(32) a32, b32, p32;\n', '      SIMDE_CONVERT_VECTOR_(a32, a_.i16);\n', '      SIMDE_CONVERT_VECTOR_(b32, b_.i16);\n', '      p32 = a32 * b32;\n', '      r_.i32 =\n', '        __builtin_shufflevector(p32, p32, 0, 2, 4, 6) +\n', '        __builtin_shufflevector(p32, p32, 1, 3, 5, 7);\n', '    #else\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_i32x4_dot_i16x8(a, b);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      b_ = simde_v128_to_private(b),\n', '      r_;\n', '\n'],
    'zarch_zvector_13_mapping': ['    #elif defined(SIMDE_ZARCH_ZVECTOR_13_NATIVE)\n', '      r_.altivec_i32 = vec_mule(a_.altivec_i16, b_.altivec_i16) + vec_mulo(a_.altivec_i16, b_.altivec_i16);\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      int32x4_t pl = vmull_s16(vget_low_s16(a_.neon_i16),  vget_low_s16(b_.neon_i16));\n', '      int32x4_t ph = vmull_s16(vget_high_s16(a_.neon_i16), vget_high_s16(b_.neon_i16));\n', '      int32x2_t rl = vpadd_s32(vget_low_s32(pl), vget_high_s32(pl));\n', '      int32x2_t rh = vpadd_s32(vget_low_s32(ph), vget_high_s32(ph));\n', '      r_.neon_i32 = vcombine_s32(rl, rh);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #if defined(SIMDE_X86_SSE2_NATIVE)\n', '      r_.sse_m128i = _mm_madd_epi16(a_.sse_m128i, b_.sse_m128i);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_i32 = vec_msum(a_.altivec_i16, b_.altivec_i16, vec_splats(0));\n']
}, {
    'name': 'simde_wasm_f32x4_ceil',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      /* https://github.com/WebAssembly/simd/pull/232 */\n', '      const __m128i input_as_i32 = _mm_cvttps_epi32(a_.sse_m128);\n', '      const __m128i i32_min = _mm_set1_epi32(INT32_MIN);\n', '      const __m128i input_is_out_of_range = _mm_or_si128(_mm_cmpeq_epi32(input_as_i32, i32_min), i32_min);\n', '      const __m128 truncated =\n', '        _mm_or_ps(\n', '          _mm_andnot_ps(\n', '            _mm_castsi128_ps(input_is_out_of_range),\n', '            _mm_cvtepi32_ps(input_as_i32)\n', '          ),\n', '          _mm_castsi128_ps(\n', '            _mm_castps_si128(\n', '              _mm_and_ps(\n', '                _mm_castsi128_ps(input_is_out_of_range),\n', '                a_.sse_m128\n', '              )\n', '            )\n', '          )\n', '        );\n', '\n', '      const __m128 trunc_is_ge_input =\n', '        _mm_or_ps(\n', '          _mm_cmple_ps(a_.sse_m128, truncated),\n', '          _mm_castsi128_ps(i32_min)\n', '        );\n', '      r_.sse_m128 =\n', '        _mm_or_ps(\n', '          _mm_andnot_ps(\n', '            trunc_is_ge_input,\n', '            _mm_add_ps(truncated, _mm_set1_ps(SIMDE_FLOAT32_C(1.0)))\n', '          ),\n', '          _mm_and_ps(trunc_is_ge_input, truncated)\n', '        );\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = simde_math_ceilf(a_.f32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_ceil(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128 = _mm_round_ps(a_.sse_m128, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);\n'],
    'type': 'simde_v128_t',
    'arm_neon_a32v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V8_NATIVE)\n', '      r_.neon_f32 = vrndpq_f32(a_.neon_f32);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE)\n', '      r_.altivec_f32 = vec_ceil(a_.altivec_f32);\n', '    #else\n']
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f64 = vrndpq_f64(a_.neon_f64);\n'],
    'name': 'simde_wasm_f64x2_ceil',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_f64 = vec_ceil(a_.altivec_f64);\n', '    #else\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = simde_math_ceil(a_.f64[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_ceil(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128d = _mm_round_pd(a_.sse_m128d, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);\n'],
    'type': 'simde_v128_t',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      /* https://github.com/WebAssembly/simd/pull/232 */\n', '\n', '      const __m128d all_but_sign_set = _mm_castsi128_pd(_mm_set1_epi64x(INT64_C(0x7FFFFFFFFFFFFFFF)));\n', '      /* https://stackoverflow.com/a/55077612 explains this a bit */\n', '      const __m128d bignum = _mm_set1_pd(4.50359962737049600000e+15);\n', '      const __m128d sign_cleared = _mm_and_pd(a_.sse_m128d, all_but_sign_set);\n', '\n', '      __m128d mask =\n', '        _mm_and_pd(\n', '          _mm_cmpnle_pd(bignum, sign_cleared),\n', '          all_but_sign_set\n', '        );\n', '      const __m128d tmp =\n', '        _mm_or_pd(\n', '          _mm_andnot_pd(mask, a_.sse_m128d),\n', '          _mm_and_pd   (mask, _mm_sub_pd(_mm_add_pd(sign_cleared, bignum), bignum))\n', '        );\n', '\n', '      r_.sse_m128d =\n', '        _mm_add_pd(\n', '          tmp,\n', '          _mm_and_pd(_mm_and_pd(_mm_cmplt_pd(tmp, a_.sse_m128d), all_but_sign_set), _mm_set1_pd(1.0))\n', '        );\n']
}, {
    'name': 'simde_wasm_f32x4_floor',
    'x86_sse2_mapping': ['    #elif defined(SIMDE_X86_SSE2_NATIVE)\n', '      const __m128i vint_min = _mm_set1_epi32(INT_MIN);\n', '      const __m128i input_as_int = _mm_cvttps_epi32(a_.sse_m128);\n', '      const __m128 input_truncated = _mm_cvtepi32_ps(input_as_int);\n', '      const __m128i oor_all_or_neg = _mm_or_si128(_mm_cmpeq_epi32(input_as_int, vint_min), vint_min);\n', '      const __m128 tmp =\n', '        _mm_castsi128_ps(\n', '          _mm_or_si128(\n', '            _mm_andnot_si128(\n', '              oor_all_or_neg,\n', '              _mm_castps_si128(input_truncated)\n', '            ),\n', '            _mm_and_si128(\n', '              oor_all_or_neg,\n', '              _mm_castps_si128(a_.sse_m128)\n', '            )\n', '          )\n', '        );\n', '      r_.sse_m128 =\n', '        _mm_sub_ps(\n', '          tmp,\n', '          _mm_and_ps(\n', '            _mm_cmplt_ps(\n', '              a_.sse_m128,\n', '              tmp\n', '            ),\n', '            _mm_set1_ps(SIMDE_FLOAT32_C(1.0))\n', '          )\n', '        );\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = simde_math_floorf(a_.f32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_floor(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'arm_neon_a32v7_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V7_NATIVE)\n', '      const int32x4_t input_as_int = vcvtq_s32_f32(a_.f32);\n', '      const float32x4_t input_truncated = vcvtq_f32_s32(input_as_int);\n', '      const float32x4_t tmp =\n', '        vbslq_f32(\n', '          vbicq_u32(\n', '            vcagtq_f32(\n', '              vreinterpretq_f32_u32(vdupq_n_u32(UINT32_C(0x4B000000))),\n', '              a_.f32\n', '            ),\n', '            vdupq_n_u32(UINT32_C(0x80000000))\n', '          ),\n', '          input_truncated,\n', '          a_.f32);\n', '      r_.neon_f32 =\n', '        vsubq_f32(\n', '          tmp,\n', '          vreinterpretq_f32_u32(\n', '            vandq_u32(\n', '              vcgtq_f32(\n', '                tmp,\n', '                a_.f32\n', '              ),\n', '              vdupq_n_u32(UINT32_C(0x3F800000))\n', '            )\n', '          )\n', '        );\n'],
    'x86_sse41_mapping': ['    #if defined(SIMDE_X86_SSE4_1_NATIVE)\n', '      r_.sse_m128 = _mm_floor_ps(a_.sse_m128);\n'],
    'type': 'simde_v128_t',
    'arm_neon_a32v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A32V8_NATIVE)\n', '      r_.neon_f32 = vrndmq_f32(a_.f32);\n'],
    'powerpc_altivec_p6_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P6_NATIVE) || defined(SIMDE_ZARCH_ZVECTOR_14_NATIVE)\n', '      r_.altivec_f32 = vec_floor(a_.altivec_f32);\n', '    #else\n']
}, {
    'simde_vectorize_mapping': ['    SIMDE_VECTORIZE\n', '    for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '      r_.f64[i] = simde_math_floor(a_.f64[i]);\n', '    }\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_floor(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f64x2_floor'
}, {
    'simde_vectorize_mapping': ['    SIMDE_VECTORIZE\n', '    for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '      r_.f32[i] = simde_math_truncf(a_.f32[i]);\n', '    }\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_trunc(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f32x4_trunc'
}, {
    'simde_vectorize_mapping': ['    SIMDE_VECTORIZE\n', '    for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '      r_.f64[i] = simde_math_trunc(a_.f64[i]);\n', '    }\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_trunc(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f64x2_trunc'
}, {
    'simde_vectorize_mapping': ['    SIMDE_VECTORIZE\n', '    for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '      r_.f32[i] = simde_math_roundf(a_.f32[i]);\n', '    }\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_nearest(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f32x4_nearest'
}, {
    'simde_vectorize_mapping': ['    SIMDE_VECTORIZE\n', '    for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '      r_.f64[i] = simde_math_round(a_.f64[i]);\n', '    }\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_nearest(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t',
    'name': 'simde_wasm_f64x2_nearest'
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f32 = vsqrtq_f32(a_.neon_f32);\n'],
    'name': 'simde_wasm_f32x4_sqrt',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_f32 = vec_sqrt(a_.altivec_f32);\n', '    #else\n'],
    'x86_sse_mapping': ['    #if defined(SIMDE_X86_SSE_NATIVE)\n', '      r_.sse_m128 = _mm_sqrt_ps(a_.sse_m128);\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f32) / sizeof(r_.f32[0])) ; i++) {\n', '        r_.f32[i] = simde_math_sqrtf(a_.f32[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f32x4_sqrt(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t'
}, {
    'arm_neon_a64v8_mapping': ['    #elif defined(SIMDE_ARM_NEON_A64V8_NATIVE)\n', '      r_.neon_f64 = vsqrtq_f64(a_.neon_f64);\n'],
    'name': 'simde_wasm_f64x2_sqrt',
    'powerpc_altivec_p7_mapping': ['    #elif defined(SIMDE_POWER_ALTIVEC_P7_NATIVE)\n', '      r_.altivec_f64 = vec_sqrt(a_.altivec_f64);\n', '    #else\n'],
    'x86_sse_mapping': ['    #if defined(SIMDE_X86_SSE_NATIVE)\n', '      r_.sse_m128d = _mm_sqrt_pd(a_.sse_m128d);\n'],
    'simde_vectorize_mapping': ['      SIMDE_VECTORIZE\n', '      for (size_t i = 0 ; i < (sizeof(r_.f64) / sizeof(r_.f64[0])) ; i++) {\n', '        r_.f64[i] = simde_math_sqrt(a_.f64[i]);\n', '      }\n', '    #endif\n', '\n', '    return simde_v128_from_private(r_);\n', '  #endif\n', '}\n'],
    'wasm': ['  #if defined(SIMDE_WASM_SIMD128_NATIVE)\n', '    return wasm_f64x2_sqrt(a);\n', '  #else\n', '    simde_v128_private\n', '      a_ = simde_v128_to_private(a),\n', '      r_;\n', '\n'],
    'type': 'simde_v128_t'
}]